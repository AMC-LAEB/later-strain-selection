{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antigenic comparison between the dominant strain and vaccine strain and reprocubily selected strains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dendropy, math, sys, json\n",
    "import pandas as pd, numpy as np\n",
    "from Bio import SeqIO\n",
    "import calendar\n",
    "from collections import deque\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "sys.path.insert(1, \"../scripts\")\n",
    "\n",
    "from labels import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. General variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_mature_protein = 17\n",
    "HA1_length_AA = 329 #in mature protein \n",
    "protein_length = 567\n",
    "sequence_length = 1701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_hemispheres = {\"us\":\"nh\", \"europe\":\"nh\", \"aunz\":\"sh\"}\n",
    "hemispheres = [\"nh\", \"sh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 24 #preceding period in months\n",
    "pdur = 16 #period duration\n",
    "flu_seasons = {h:{} for h in hemispheres}\n",
    "vaccine_selection = {h:{} for h in hemispheres}\n",
    "for y in range(2002, 2024):\n",
    "    #northern hemisphere flu season\n",
    "    season = f\"{y}-{y+1}\"\n",
    "    if y != 2023:\n",
    "        flu_seasons[\"nh\"][season] = (date(y, 10, 1), date(y+1, 4, 30))\n",
    "        #northern hemisphere vaccine strain selection moment\n",
    "        vaccine_selection[\"nh\"][season] = date(y,2,1)  \n",
    "    \n",
    "    if y==2002:\n",
    "        continue\n",
    "    #southern hemisphere flu season\n",
    "    flu_seasons[\"sh\"][str(y)] = (date(y, 3, 1), date(y, 9, 30))\n",
    "    #southern hemisphere vaccine strain selection moment\n",
    "    vaccine_selection[\"sh\"][str(y)] = date(y-1,9,1)\n",
    "\n",
    "season_periods = {h:{} for h in hemispheres}\n",
    "period_dates = {}\n",
    "for h, sd in flu_seasons.items():\n",
    "    for season, (fss, fse) in sd.items():\n",
    "        vsd = vaccine_selection[h][season]\n",
    "        #something with periods in treason doesn't seem to be correct but don't wanna waste time on that for now\n",
    "        if h == \"nh\":\n",
    "            ps, pe  = vsd + relativedelta(months=-pp), vsd + relativedelta(months=+pdur-1)\n",
    "        else:\n",
    "            ps, pe  = vsd + relativedelta(months=-pp), vsd + relativedelta(months=+pdur-1)\n",
    "\n",
    "        \n",
    "        period = f\"{str(ps.year)[2:]}{'0'+str(ps.month) if len(str(ps.month))==1 else str(ps.month)}-{str(pe.year)[2:]}{'0'+str(pe.month) if len(str(pe.month))==1 else str(pe.month)}\"\n",
    "        season_periods[h][season] = period\n",
    "        period_dates[period] = (ps, pe)\n",
    "\n",
    "#cut off for HA1 \n",
    "early_season_cutoff = {'nh':'2011-2012', 'sh':'2011'}\n",
    "early_season_cutoff_dates = {h:flu_seasons[h][season][-1] for h, season in early_season_cutoff.items()}\n",
    "early_seasons = [] #get list\n",
    "for h, cutoff in early_season_cutoff.items():\n",
    "    csons = list(flu_seasons[h].keys())\n",
    "    for i, season in enumerate(csons):\n",
    "        if i <= csons.index(cutoff):\n",
    "            early_seasons.append(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epitope_sites= {\"A\":[122,124,126,130,131,132,133,135,137,138,140,142,143,144,145,146,150,152,168], \n",
    "                \"B\":[128,129,155,156,157,158,159,163,165,186,187,188,189,190,192,193,194,196,197,198], \n",
    "                \"C\":[44,45,46,47,48,50,51,53,54,273,275,276,278,279,280,294,297,299,300,304,305,307,308,309,310,311,312], \n",
    "                \"D\":[96,102,103,117,121,167,170,171,172,173,174,175,176,177,179,182,201,203,207,208,209,212,213,214,215,216,217,218,219,226,227,228,229,230,238,240,242,244,246,247,248,], \n",
    "                \"E\":[57,59,62,63,67,75,78,80,81,82,83,86,87,88,91,92,94,109,260,261,262,265]}\n",
    "epitope_positions = [s for sites in epitope_sites.values() for s in sites]\n",
    "\n",
    "#rbs\n",
    "rbs_positions = [98, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 144, 145, 146, 153, 154, 155, 156, 157, 158, 159,\n",
    "                 183, 184, 185, 186, 187, 188, 189, 190, 190, 191, 192, 193, 194, 195, 196, 219, 220, 221, 222, 223, 224, \n",
    "                 225, 226, 227, 228,]\n",
    "#rbs_positions = [131, 132, 133, 134, 135, 136, 137, 138, 140, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228,]\n",
    "koel_sites = [145, 189, 193, 156, 159, 158, 155]\n",
    "\n",
    "#amino acid list for coding \n",
    "aa_list = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_antigenic_similar_sequences(seq, seqdict):\n",
    "    \"\"\"returns list of strain names of closest sequences in seqdict and the number of diffences between them \"\"\"\n",
    "    min_diff = len(seq)\n",
    "    cseq = []\n",
    "    \n",
    "    for strain, s in seqdict.items():\n",
    "        ndiff= len([p for p in epitope_positions if seq[p-1]!=s[p-1]])\n",
    "        if ndiff < min_diff:\n",
    "            min_diff = ndiff\n",
    "            cseq = [strain]\n",
    "        elif ndiff == min_diff:\n",
    "            cseq.append(strain)\n",
    "\n",
    "    return cseq, min_diff\n",
    "\n",
    "def get_most_similar_sequences(seq, seqdict):\n",
    "    \"\"\"returns list of strain names of closest sequences in seqdict and the number of diffences between them \"\"\"\n",
    "    min_diff = len(seq)\n",
    "    cseq = []\n",
    "    \n",
    "    for strain, s in seqdict.items():\n",
    "        ndiff= len([i for i in range(len(seq))if seq[i]!=s[i]])\n",
    "        if ndiff < min_diff:\n",
    "            min_diff = ndiff\n",
    "            cseq = [strain]\n",
    "        elif ndiff == min_diff:\n",
    "            cseq.append(strain)\n",
    "\n",
    "    return cseq, min_diff\n",
    "\n",
    "def can_merge_tables(tables, merge_dict):\n",
    "    visited = set()\n",
    "    queue = deque([tables[0]])\n",
    "\n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        if current in visited:\n",
    "            continue\n",
    "        visited.add(current)\n",
    "        for neighbor in merge_dict.get(current, []):\n",
    "            if neighbor in tables and neighbor not in visited:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    return visited == set(tables)\n",
    "\n",
    "def count_items_in_tables(tables, table_lists, table_strains):\n",
    "    counts = []\n",
    "    for lst, mapping in zip(table_lists, table_strains):\n",
    "        items_in_tables = set()\n",
    "        for table in tables:\n",
    "            items_in_tables.update(mapping.get(table, []))\n",
    "        counts.append(len(items_in_tables.intersection(lst)))\n",
    "    return counts\n",
    "\n",
    "def write_antigens(fw, mapcode, ag_list, ag_name):\n",
    "    #there must be a better way to do this but this works\n",
    "    if len(ag_list) <= 10:\n",
    "        if len(ag_list) == 1:\n",
    "            fw.write(f'{ag_name} <- agNames(map{mapcode}) %in% c(\"{ag_list[0]}\")\\n')\n",
    "        else:\n",
    "            fw.write(f'{ag_name} <- agNames(map{mapcode}) %in% c{tuple(ag_list)}\\n')\n",
    "    else:\n",
    "        l = ','.join([f'\"{ag}\"' for ag in ag_list[0:10]])\n",
    "        fw.write(f\"{ag_name} <- agNames(map{mapcode}) %in% c({l},\\n\")\n",
    "        if len(range(10, len(ag_list)-10, 10)) > 0:\n",
    "            for i in range(10, len(ag_list)-10, 10):\n",
    "                l = ','.join([f'\"{ag}\"' for ag in ag_list[i:i+10]])\n",
    "                if i > len(ag_list)-10:\n",
    "                    fw.write(f\"\\t{l})\\n\")\n",
    "                else:\n",
    "                    fw.write(f\"\\t{l},\\n\")\n",
    "            try:\n",
    "                if i < len(ag_list)-10:\n",
    "                    l = ','.join([f'\"{ag}\"' for ag in ag_list[i+10:]])\n",
    "                    fw.write(f\"\\t{l})\\n\")\n",
    "            except:\n",
    "                l = ','.join([f'\"{ag}\"' for ag in ag_list[10:]])\n",
    "                fw.write(f\"'t{l})\\n\")\n",
    "\n",
    "        else:\n",
    "            l = ','.join([f'\"{ag}\"' for ag in ag_list[10:]])\n",
    "            fw.write(f\"\\t{l})\\n\")\n",
    "\n",
    "def centroid(points):\n",
    "    x = [p[0] for p in points]\n",
    "    y= [p[1] for p in points]\n",
    "    l = len(points)\n",
    "    centroid_x = sum(x)/l\n",
    "    centroid_y = sum(y)/l\n",
    "    return [centroid_x, centroid_y]\n",
    "\n",
    "def get_mutations(ref, seq, sl=\"complete\"):\n",
    "    muts = []\n",
    "    for i, b in enumerate(ref):\n",
    "        if seq[i] == \"X\" or b == \"X\":\n",
    "            continue \n",
    "        if seq[i] != b:\n",
    "            if sl == \"HA1\":\n",
    "                pos = start_mature_protein+i\n",
    "                mpos = i+1\n",
    "                protein = \"HA1\"\n",
    "            else: # if sl==\"complete\"\n",
    "                pos = i+1\n",
    "                mpos = pos-start_mature_protein+1\n",
    "                protein = \"signal protein\" if pos<start_mature_protein else \"HA1\" if pos<=HA1_length_AA else \"HA2\"\n",
    "            ep = True if mpos in epitope_positions else False\n",
    "            rbs = True if mpos in rbs_positions else False\n",
    "            if ep:\n",
    "                for s, pos_list in epitope_sites.items():\n",
    "                    if mpos in pos_list:\n",
    "                        site = s\n",
    "            else:\n",
    "                site = pd.NA\n",
    "            muts.append([b, seq[i], pos, mpos, protein, ep, site, rbs])\n",
    "    muts = pd.DataFrame.from_records(muts, columns=[\"actual strain AA\", \"antigen strain AA\", \"position\", \"mature position\", \"protein\", \"epitope\", \"epitope site\", \"RBS\"])\n",
    "    return muts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. WHO vaccine strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaccine strains per hemisphere per season year\n",
    "#egg-based strain recommendation as listed on the WHO website\n",
    "vaccine_strain_names = {'nh': {'2024-2025':'A/Thailand/8/2022', '2023-2024':'A/Darwin/9/2021', '2022-2023':'A/Darwin/9/2021', '2021-2022':'A/Cambodia/e0826360/2020',\n",
    "                               '2020-2021':'A/Hong Kong/2671/2019', '2019-2020':'A/Kansas/14/2017', '2018-2019':'A/Singapore/INFIMH-16-0019/2016', '2017-2018':'A/Hong Kong/4801/2014',\n",
    "                               '2016-2017':'A/Hong Kong/4801/2014','2015-2016':'A/Switzerland/9715293/2013','2014-2015':'A/Texas/50/2012','2013-2014':'A/Texas/50/2012',\n",
    "                               '2012-2013':'A/Victoria/361/2011','2011-2012':'A/Perth/16/2009','2010-2011':'A/Perth/16/2009','2009-2010':'A/Brisbane/10/2007',\n",
    "                               '2008-2009':'A/Brisbane/10/2007','2007-2008':'A/Wisconsin/67/2005','2006-2007':'A/Wisconsin/67/2005','2005-2006':'A/California/7/2004',\n",
    "                               '2004-2005':'A/Fujian/411/2002','2003-2004':'A/Moscow/10/99','2002-2003':'A/Moscow/10/99','2001-2002':'A/Moscow/10/99','2000-2001':'A/Moscow/10/99'},\n",
    "                        'sh': {'2024':'A/Thailand/8/2022','2023':'A/Darwin/9/2021','2022':'A/Darwin/9/2021','2021':'A/Hong Kong/2671/2019','2020':'A/South Australia/34/2019',\n",
    "                               '2019':'A/Switzerland/8060/2017','2018':'A/Singapore/INFIMH-16-0019/2016','2017':'A/Hong Kong/4801/2014','2016':'A/Hong Kong/4801/2014',\n",
    "                               '2015':'A/Switzerland/9715293/2013','2014':'A/Texas/50/2012','2013':'A/Victoria/361/2011','2012':'A/Perth/16/2009','2011':'A/Perth/16/2009',\n",
    "                               '2010':'A/Perth/16/2009','2009':'A/Brisbane/10/2007','2008':'A/Brisbane/10/2007','2007':'A/Wisconsin/67/2005','2006':'A/California/7/2004',\n",
    "                               '2005':'A/Wellington/1/2004','2004':'A/Fujian/411/2002','2003':'A/Moscow/10/99','2002':'A/Moscow/10/99','2001':'A/Moscow/10/99',\n",
    "                               '2000':'A/Moscow/10/99'}\n",
    "                        }\n",
    "\n",
    "vaccine_strain_dir = \"../data/vaccine_strains/H3N2\"\n",
    "\n",
    "vaccine_strains = {}\n",
    "for f in os.listdir(vaccine_strain_dir):\n",
    "    strain = f.split(\".\")[0].split(\"_\")\n",
    "    strain[1] = strain[1].replace(\"-\", \" \")\n",
    "    strain = \"/\".join(strain)\n",
    "    \n",
    "    #get full cds and amino acid sequence \n",
    "    cds = [r for r in SeqIO.parse(os.path.join(vaccine_strain_dir, f), \"fasta\")][0].seq[:1701]\n",
    "    aa_seq = cds.translate()\n",
    "    aa_seq = aa_seq[:protein_length]\n",
    "\n",
    "    #get proteins - signal protein\n",
    "    signal_prot_nuc = cds[:(start_mature_protein*3)-1] \n",
    "    signal_prot_aa = aa_seq[:(start_mature_protein)-1]\n",
    "    #HA1 \n",
    "    HA1_nuc = cds[(start_mature_protein*3)-1:((start_mature_protein*3)+(HA1_length_AA*3))-1]\n",
    "    HA1_aa = aa_seq[start_mature_protein-1:+start_mature_protein+HA1_length_AA-1]\n",
    "    #HA2\n",
    "    HA2_nuc = cds[((start_mature_protein*3)-1+(HA1_length_AA*3)):]\n",
    "    HA2_aa = aa_seq[start_mature_protein-1+HA1_length_AA:]\n",
    "\n",
    "    vaccine_strains[strain] = {\"nuc\":{\"complete\":cds, \"signal protein\":signal_prot_nuc, \"HA1\":HA1_nuc, \"HA2\":HA2_nuc, \"mature protein\":HA1_nuc+HA2_nuc},\n",
    "                               \"aa\":{\"complete\":aa_seq, \"signal protein\":signal_prot_aa, \"HA1\":HA1_aa, \"HA2\":HA2_aa, \"mature protein\":HA1_aa+HA2_aa}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dominant strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_strain_file = '../data/dominant_strains.fasta'\n",
    "dominant_strains = []\n",
    "for r in SeqIO.parse(dominant_strain_file, \"fasta\"):\n",
    "    region, season = r.id.split(\"_\")[0], r.id.split(\"_\")[1]\n",
    "    dominant_strains.append([region, season, r.seq])\n",
    "\n",
    "dominant_strains = pd.DataFrame.from_records(dominant_strains, columns=[\"region\", \"season\", \"sequence\"]).set_index([\"region\", \"season\"]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Reproducible selection strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproducible_selection_strain_file = \"../data/reproducible_selection_strains.fasta\"\n",
    "reproducible_selection_strains = []\n",
    "for r in SeqIO.parse(reproducible_selection_strain_file, \"fasta\"):\n",
    "    region, season, months, timing = r.id.split(\"_\")\n",
    "    reproducible_selection_strains.append([region, season, months, timing, r.seq])\n",
    "\n",
    "reproducible_selection_strains = pd.DataFrame.from_records(reproducible_selection_strains, columns=[\"region\", \"season\", \"months\", \"timing\", \"sequence\"]).set_index([\"region\", \"season\", \"timing\"]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. HI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mill hill data\n",
    "HI_file = pd.ExcelFile(\"../data/HI_data/HI_titer_extracted.xlsx\")\n",
    "general_columns = [\"date\", \"passage\"]\n",
    "known_passage = [\"Cell\", \"SIAT\", \"MDCK\", \"Egg\"]\n",
    "HI_titers = []\n",
    "for i, sn in enumerate(HI_file.sheet_names):\n",
    "    df = HI_file.parse(sn, index_col=0)\n",
    "    for strain, row in df.iterrows():\n",
    "        l = [strain]\n",
    "\n",
    "        for gc in general_columns:\n",
    "            #add general columns\n",
    "            if gc in df.columns:\n",
    "                if pd.isna(row[gc]) or row[gc]==\"unknown\":\n",
    "                    l.append(\"*\")\n",
    "                elif gc == \"date\":\n",
    "                    l.append(str(row[gc].date()))\n",
    "                else:\n",
    "                    l.append(str(row[gc]))\n",
    "\n",
    "            elif gc == \"date\":\n",
    "                l.append(\"*\")\n",
    "            else:\n",
    "                l.append(\"*\")\n",
    "\n",
    "        #add HI strains \n",
    "        for his in df.columns:\n",
    "            \n",
    "            titre = row[his]\n",
    "            if titre == \"ND\" or titre==\"NT\" or pd.isna(titre):\n",
    "                titre = \"*\"\n",
    "            if titre == \"<\":\n",
    "                titre = \"<40\"\n",
    "                \n",
    "            if his not in general_columns:\n",
    "                if len(his.split(\" \")) == 1: #no additional information (cell type or ferret number)\n",
    "                    #HI_titers.append(l+[his,pd.NA,pd.NA,titre,year,table])\n",
    "                    HI_titers.append([\"|\".join(l)] + [f\"{his}|*|*\", titre,sn])\n",
    "\n",
    "                else:\n",
    "                    his = his.split(\" \")\n",
    "                    #check if there was a space in the strain name\n",
    "                    if not his[1].startswith(\"F\") and \"/\" in his[1]:\n",
    "                        his[0] = f'{his[0]} {his[1]}'\n",
    "                        his =[his[0]] + his[2:] #reassign list\n",
    "                    his = [v for v in his if len(v)>0]\n",
    "                    if len(his) ==1:\n",
    "                        # HI_titers.append(l+[his[0],pd.NA,pd.NA,titre,year,table])\n",
    "                        HI_titers.append([\"|\".join(l)]+[f\"{his[0]}|*|*\",titre,sn])\n",
    "                        \n",
    "\n",
    "                    else:\n",
    "                        #determine if there is a passage citation\n",
    "                        if his[1] not in known_passage:\n",
    "                            # HI_titers.append(l+[his[0],pd.NA,\" \".join(his[1:]),titre,year,table])\n",
    "                            HI_titers.append([\"|\".join(l)]+[f\"{his[0]}|*|\"+\" \".join(his[1:]),titre,sn])\n",
    "                        else:\n",
    "                            # HI_titers.append(l+[his[0],his[1],' '.join(his[2:]),titre,year,table])\n",
    "                            HI_titers.append([\"|\".join(l)]+[f\"{his[0]}|{his[1]}|\"+' '.join(his[2:]),titre,sn])\n",
    "\n",
    "HI_titers = pd.DataFrame.from_records(HI_titers, columns=[\"antigen\", \"antiserum\",  \"HI titer\", \"table\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/HI_data/HI_table_types.json\") as f:\n",
    "    HI_table_types = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign table type, strain, passage type\n",
    "\n",
    "passage_labels = {\"egg\":epl, \"cell\":cbpl+ocbpl, \"unknown\":[' ', '0_1', '1.Pa(RKI)_1', '1_1', '320', 'HGR NIB']}\n",
    "for i, row in HI_titers.iterrows():\n",
    "    HI_titers.loc[i,\"type\"] = HI_table_types[row[\"table\"]]\n",
    "\n",
    "    HI_titers.loc[i, \"strain\"] = row[\"antigen\"].split(\"|\")[0].replace(\"*\", \"\")\n",
    "\n",
    "    psl = row[\"antigen\"].split(\"|\")[-1]\n",
    "    if psl == \"*\":\n",
    "        HI_titers.loc[i, \"passage\"] = \"unknown\"\n",
    "        \n",
    "\n",
    "    for p, pl in passage_labels.items():\n",
    "        if psl in pl:\n",
    "            HI_titers.loc[i, \"passage\"] = p\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/HI_data/HI_titer_merge.json\") as f:\n",
    "    HI_table_merge = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1. individual HI tables \n",
    "need these to constructed antigenic maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double antigens or antiserums in Feb2018_table8-5\n"
     ]
    }
   ],
   "source": [
    "#to write \n",
    "outpath = \"../data/HI_data/individual_HI_tables\"\n",
    "for table in HI_titers[\"table\"].unique():\n",
    "    df = HI_titers[HI_titers[\"table\"]==table][[\"antigen\", \"antiserum\", \"HI titer\"]]\n",
    "    df[\"antigen\"] = [i.replace(\",\",\"\") for i in df[\"antigen\"]]\n",
    "    df[\"antiserum\"] = [i.replace(\",\",\"\") for i in df[\"antiserum\"]]\n",
    "    \n",
    "    try:\n",
    "        df = df.pivot(index=\"antigen\", columns=\"antiserum\", values=\"HI titer\")\n",
    "        df.index.name = None\n",
    "        df.to_csv(os.path.join(outpath, f\"{table}.csv\"))\n",
    "    except:\n",
    "       print (\"double antigens or antiserums in\", table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Sequence data \n",
    "to find closest match with HI antigens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get raw unaligned sequences\n",
    "#get raw data\n",
    "raw_data_dir = \"../data/gisaid_data/raw_downloads/H3N2_HA/\"\n",
    "raw_sequences, raw_metadata= {}, []\n",
    "for f in os.listdir(raw_data_dir):\n",
    "    if f.endswith(\".fasta\"):\n",
    "        for r in SeqIO.parse(os.path.join(raw_data_dir, f), \"fasta\"):\n",
    "            raw_sequences[r.id.split(\"|\")[0]] = r.seq\n",
    "    elif f.endswith(\".xls\"):\n",
    "        try:\n",
    "            raw_metadata = pd.concat([raw_metadata, pd.read_excel(os.path.join(raw_data_dir, f), usecols=[\"Isolate_Id\",\"HA Segment_Id\", \"Isolate_Name\", \"Passage_History\", \"Location\", \"Collection_Date\"])])\n",
    "            raw_metadata = raw_metadata.drop_duplicates().reset_index(drop=True)\n",
    "        except:\n",
    "            raw_metadata = pd.read_excel(os.path.join(raw_data_dir, f), usecols=[\"Isolate_Id\",\"HA Segment_Id\", \"Isolate_Name\", \"Passage_History\", \"Location\", \"Collection_Date\"])\n",
    "\n",
    "#parse collection date as date\n",
    "raw_metadata[\"Collection_Date\"] =pd.to_datetime(raw_metadata[\"Collection_Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "#deterime passage history \n",
    "for i, row in raw_metadata.iterrows():\n",
    "    ph = row[\"Passage_History\"]\n",
    "    if ph in cpl:\n",
    "        raw_metadata.loc[i, \"Passage_History\"] = \"clinical\"\n",
    "    elif ph in cbpl:\n",
    "        raw_metadata.loc[i, \"Passage_History\"] = \"cell-based (MDCK or SIAT)\"\n",
    "    elif ph in epl:\n",
    "        raw_metadata.loc[i, \"Passage_History\"] = \"egg-based\"\n",
    "    elif ph in ocbpl:\n",
    "        raw_metadata.loc[i, \"Passage_History\"] = \"cell-based (other)\"\n",
    "    else:\n",
    "        raw_metadata.loc[i, \"Passage_History\"] = \"unknown or unclear\"\n",
    "\n",
    "#determine hemisphere and country\n",
    "for i, row in raw_metadata.iterrows():\n",
    "    l = row[\"Location\"]\n",
    "    #determine country\n",
    "    try:\n",
    "        country = l.split(\" / \")[1]\n",
    "    except:\n",
    "        country = pd.NA\n",
    "    country = cs[country] if country in cs.keys() and not pd.isna(country) else country\n",
    "    if not pd.isna(country) and country.upper() == country:\n",
    "        country = country.lower().capitalize()\n",
    "    raw_metadata.loc[i, \"country\"] = country\n",
    "    \n",
    "    #determine hemisphere\n",
    "    if not pd.isna(country):\n",
    "        if country in nhc:\n",
    "            raw_metadata.loc[i, \"hemisphere\"] = \"northern\"\n",
    "        elif country in shc:\n",
    "            raw_metadata.loc[i, \"hemisphere\"] = \"southern\"\n",
    "        else:\n",
    "            print (country)\n",
    "            raw_metadata.loc[i, \"hemisphere\"] = pd.NA\n",
    "    else:\n",
    "        raw_metadata.loc[i, \"hemisphere\"] = pd.NA\n",
    "\n",
    "#sort by collection date\n",
    "raw_metadata = raw_metadata.sort_values([\"Collection_Date\"]).reset_index(drop=True)\n",
    "\n",
    "#remove sequences with unknown country \n",
    "raw_metadata = raw_metadata.dropna().reset_index(drop=True)     \n",
    "\n",
    "# check for valid nucleotides in sequences\n",
    "valid_nucs = ['A','T','C','G','R','Y','B','D','K','M','H','V','S','W','N']\n",
    "weird_nucs = []\n",
    "for sid, seq in raw_sequences.items():\n",
    "    if not all(i in valid_nucs for i in seq.upper()):\n",
    "        weird_nucs.append(sid.split(\"|\")[0])\n",
    "# metadata = metadata[~metadata[\"Isolate_Id\"].isin(found_outliers + weird_nucs)]\n",
    "raw_metadata = raw_metadata[~raw_metadata[\"Isolate_Id\"].isin(weird_nucs)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sequences for HI antigens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 match antigens to GISAID strain names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get ids of HI strains\n",
    "known_expections = {\"NYMC X-327(A/Kansas/14/17)\":\"A/Kansas/14/2017\",\"NYMC X-327 (A/Kansas/14/17)\":\"A/Kansas/14/2017\"}\n",
    "tested_antigens = HI_titers[\"strain\"].unique().tolist()\n",
    "\n",
    "tested_antigen_ids = {}\n",
    "antigens_without_gisaid_seq = []\n",
    "for strain in tested_antigens:\n",
    "    if strain in tested_antigen_ids.keys():\n",
    "        continue\n",
    "   \n",
    "    try:\n",
    "        li = [strain.split(\" \").index(i) for i in strain.split(\" \") if \"/\" in i][-1]\n",
    "    except:\n",
    "        break\n",
    "    s  = \" \".join([i for i in strain.split(\" \") if strain.split(\" \").index(i) <= li])\n",
    "\n",
    "    if s in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==s][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "\n",
    "    if s in known_expections.keys():\n",
    "        ns = known_expections[s]\n",
    "        if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "            tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "            continue\n",
    "\n",
    "    #replace spaces\n",
    "    ns = s.replace(\" \", \"\")\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "\n",
    "    ns = s.replace(\" \", \"_\")\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "\n",
    "    ns = s.replace(\"_\", \" \")\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "\n",
    "    ns = s.replace(\"&\", \"and\")\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "    ns = s.replace(\"_\", \" \")\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "\n",
    "    ns = s.replace(\"_\", \" \")\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "\n",
    "    #try full year\n",
    "    if len(s.split(\"/\")[-1])== 2:\n",
    "        ns = \"/\".join(s.split(\"/\")[:-1] + [\"20\"+s.split(\"/\")[-1]])\n",
    "        if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "            tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "            continue\n",
    "\n",
    "    if len(s.split(\"/\")[-1])== 2:\n",
    "        ns = \"/\".join(s.split(\"/\")[:-1] + [\"19\"+s.split(\"/\")[-1]])\n",
    "        if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "            tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "            continue\n",
    "\n",
    "    ns = s.replace(\"-\", \" \")\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "\n",
    "    ns = s.replace(\"-\", \"\")\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "        \n",
    "    ns = s.split(\" \")[0]\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "\n",
    "    ns = s.split(\" (\")[-1].rstrip(\")\")\n",
    "    ns = ns.split(\"_(\")[-1].rstrip(\")\")\n",
    "    ns = ns.lstrip(\"hy \")\n",
    "    if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "        tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "        continue\n",
    "\n",
    "        \n",
    "    #try shortened year\n",
    "    if len(s.split(\"/\")[-1])>2:\n",
    "        ns = \"/\".join(s.split(\"/\")[:-1] + [s.split(\"/\")[-1][-2:]])\n",
    "        if ns in raw_metadata[\"Isolate_Name\"].to_list():\n",
    "            tested_antigen_ids[strain] = raw_metadata[raw_metadata[\"Isolate_Name\"]==ns][\"Isolate_Id\"].values[0]\n",
    "            continue\n",
    "    \n",
    "    antigens_without_gisaid_seq.append(strain)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HI_selection_dir = \"../analysis/antigenic_comparison/HI_data\"\n",
    "if not os.path.isdir(HI_selection_dir):\n",
    "    os.mkdir(HI_selection_dir)\n",
    "\n",
    "#write output file for alignment \n",
    "HI_sequence_file = os.path.join(HI_selection_dir, \"HI_antigen_sequences.fasta\")\n",
    "with open(HI_sequence_file, \"w\") as fw:\n",
    "    for strain, iid in tested_antigen_ids.items():\n",
    "        fw.write(f\">{strain}\\n{raw_sequences[iid]}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Align sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads =12\n",
    "ref_ha = \"../data/references/H3N2_HA.fasta\"\n",
    "\n",
    "\n",
    "HI_alignment_file = os.path.join(HI_selection_dir, \"HI_antigen_seq_alignment.fasta\")\n",
    "if not os.path.isfile(HI_alignment_file): \n",
    "    cmd = ['mafft', '--auto', '--thread', str(threads), '--keeplength', '--addfragments', HI_sequence_file , ref_ha,'>', HI_alignment_file]\n",
    "    os.system(\" \".join(cmd))\n",
    "\n",
    "\n",
    "HI_antigen_seqs = {} #complete sequences\n",
    "for r in SeqIO.parse(HI_alignment_file, \"fasta\"):\n",
    "    if r.id == \"KX879573.1-ref_1968-A/Alchi/2/1968|4-HA\": #skip reference\n",
    "        continue\n",
    "    strain = r.description\n",
    "    AA_seq = r.seq[:sequence_length].replace(\"-\", \"N\").translate()\n",
    "    HI_antigen_seqs[strain] = AA_seq\n",
    "\n",
    "HI_antigen_seqs = {strain:seq for strain, seq in HI_antigen_seqs.items() if strain in HI_titers[\"strain\"].unique().tolist()}\n",
    "HA1_HI_antigen_seqs = {strain:seq[start_mature_protein-1:HA1_length_AA+start_mature_protein-1] for strain, seq in HI_antigen_seqs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get most similar HI strains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_HI_strains =[]\n",
    "diff_HI_selected = []\n",
    "\n",
    "for region, h in region_hemispheres.items():\n",
    "    for season, (fss, fse) in flu_seasons[h].items():\n",
    "\n",
    "        ds = dominant_strains.loc[(region, season), \"sequence\"]\n",
    "        vs = vaccine_strains[vaccine_strain_names[h][season]][\"aa\"][\"HA1\" if season in early_seasons else \"complete\"]\n",
    "        css = reproducible_selection_strains.loc[(region, season, \"WHO-timing\"), \"sequence\"]\n",
    "        lss = reproducible_selection_strains.loc[(region, season, \"delayed-timing\"), \"sequence\"]\n",
    "\n",
    "        for strain, seq in {\"dominant strain\": ds, \"vaccine strain\": vs, \"reproducible selection at WHO-timing\":css,\n",
    "                            \"reproducible selection at delayed timing\":lss}.items():\n",
    "            \n",
    "            #for early season GETTING THE MOST ANTIGENICALLY SIMILAR SEQUENCES\n",
    "            if season  in early_seasons:\n",
    "            \n",
    "                acseq, adiff = get_most_antigenic_similar_sequences(seq, HA1_HI_antigen_seqs)\n",
    "\n",
    "                #prioritise passage types\n",
    "                pt = [HI_titers[HI_titers[\"strain\"]==s][\"passage\"].values[0] for s in acseq]\n",
    "                if \"cell\" in pt:\n",
    "                    acseq = [acseq[i] for i, p in enumerate(pt) if p == \"cell\"]\n",
    "                elif \"egg\" in pt:\n",
    "                    acseq = [acseq[i] for i, p in enumerate(pt) if p == \"egg\"]\n",
    "\n",
    "                \n",
    "                closest_HI_strains.append([region, season, strain, \", \".join(acseq)])\n",
    "                diff_HI_selected.append([region, season, strain, adiff])\n",
    "\n",
    "            #for later season GETTING THE GENETICALLY MOST SIMILAR SEQUENCES   \n",
    "            else:\n",
    "                #prioritizing smallest difference in the antigenic sites\n",
    "                acseq, adiff = get_most_antigenic_similar_sequences(seq, HI_antigen_seqs)\n",
    "                cseq, diff = get_most_similar_sequences(seq, {strain:seq for strain, seq in HI_antigen_seqs.items() if strain in acseq})\n",
    "\n",
    "                pt = [HI_titers[HI_titers[\"strain\"]==s][\"passage\"].values[0] for s in cseq]\n",
    "                if \"cell\" in pt:\n",
    "                    cseq = [cseq[i] for i, p in enumerate(pt) if p == \"cell\"]\n",
    "                elif \"egg\" in pt:\n",
    "                    cseq = [cseq[i] for i, p in enumerate(pt) if p == \"egg\"]\n",
    "\n",
    "                closest_HI_strains.append([region, season, strain, \", \".join(cseq)])\n",
    "                diff_HI_selected.append([region, season, strain, diff])\n",
    "\n",
    "closest_HI_strains = pd.DataFrame.from_records(closest_HI_strains, columns=[\"region\", \"season\", \"strain\", \"HI antigens\"]).set_index([\"region\", 'season']).sort_index()\n",
    "diff_HI_selected = pd.DataFrame.from_records(diff_HI_selected, columns=[\"region\", \"season\", \"strain\", \"number of differences\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HI tables to create antigenic maps per regional season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. manually find tables to merge to create antigenic maps\n",
    "quickest way to do this was manually....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual selection dict with list of selected HI tables\n",
    "manual_HI_table_selection = {('aunz', '2003'):[[\"2003_table5\", \"2003_table6\"], [\"2003_table5\"], [\"2003_table6\"]],\n",
    "                             ('aunz', '2004'):[[]], \n",
    "                             ('aunz', '2005'):[[]],\n",
    "                             ('aunz', '2006'):[[\"Mar2007_table4\", \"Mar2007_table5\", \"Mar2006_table4\", \"Sep2006_table4\", 'Sep2006_table6'],[\"Mar2007_table4\"], [\"Mar2007_table5\"]],\n",
    "                             ('aunz', '2007'):[[]],\n",
    "                             ('aunz', '2008'):[[\"Sep2008_table4A\", \"Sep2008_table4B\", \"Sep2007_table8\"], [\"Sep2008_table4B\", \"Sep2008_table4A\", \"Sep2007_table7\", \"Sep2007_table6\"], \n",
    "                                               [\"Feb2009_table4\"],  [\"Sep2008_table4A\", \"Mar2008_table5\"],],\n",
    "                             ('aunz', '2009'):[[\"Sep2009_table6\"]],\n",
    "                             ('aunz', '2010'):[[\"Sep2010_table7B\", \"Feb2010_table13\"], [\"Sep2009_table5\", \"Sep2010_table7B\", \"Feb2010_table13\"]],\n",
    "                             ('aunz', '2011'):[[\"Sep2011_table18\"], [\"Sep2012_table9\"],[\"Feb2012_table12\", \"Feb2012_table13\"], [\"Sep2012_table15\"],[\"Feb2012_table10\"]],\n",
    "                             ('aunz', '2012'):[[\"Feb2012_table13\"], [\"Feb2012_table14\"], [ \"Sep2012_table18\", \"Sep2012_table17\"], [\"Sep2012_table17\", \"Sep2012_table12\"],\n",
    "                                               [\"Feb2012_table9\", \"Sep2012_table10\"]],\n",
    "                             ('aunz', '2013'):[[\"Sep2013_table7-7\",\"Sep2013_table7-10\",\"Feb2012_table13\"],[\"Feb2013_table18\", \"Feb2012_table13\"], [\"Feb2012_table13\", \"Sep2013_table7-16\", \"Sep2013_table7-11\"], \n",
    "                                               [\"Sep2012_table20\", \"Sep2013_table7-12\", \"Sep2013_table7-9\"], [\"Sep2012_table20\", \"Feb2013_table18\", \"Feb2013_table20\"]],\n",
    "                             ('aunz', '2014'):[[\"Sep2012_table9\", \"Feb2014_table9-8\", 'Sep2013_table7-8'], [\"Sep2012_table9\", \"Sep2013_table7-7\", \"Feb2014_table9-2\", \"Sep2014_table8-10\", \"Sep2014_table8-14\"], \n",
    "                                               [\"Sep2012_table15\",\"Sep2013_table7-12\", \"Feb2014_table9-7\", 'Sep2014_table8-14'], [\"Sep2012_table15\",\"Sep2013_table7-12\", \"Feb2014_table9-2\", 'Feb2014_table9-8'],\n",
    "                                               [\"Sep2012_table15\",\"Sep2013_table7-7\", \"Feb2014_table9-7\", 'Feb2014_table9-8', \"Sep2014_table8-14\"]],\n",
    "                             ('aunz', '2015'):[[\"Feb2015_table9-10\", \"Feb2015_table9-4\", \"Sep2014_table8-18\"],[\"Sep2015_table8-9\", \"Feb2015_table9-10\", \"Sep2014_table8-19\"], \n",
    "                                               [\"Sep2014_table8-19\",\"Sep2014_table8-18\",\"Sep2014_table8-10\"]],\n",
    "                             ('aunz', '2016'):[[\"Sep2017_table8-13\", \"Sep2017_table8-12\", \"Sep2016_table8-6\",\"Sep2016_table8-2\"], [\"Sep2017_table8-13\",\"Feb2016_table9-3\",\"Feb2017_table8-5\"],\n",
    "                                               [\"Sep2017_table8-16\", \"Sep2017_table8-2\", \"Sep2016_table8-2\"], [\"Sep2017_table8-13\", \"Sep2017_table8-2\", \"Sep2016_table8-2\"],\n",
    "                                               [\"Sep2017_table8-13\", \"Sep2017_table8-2\",\"Feb2016_table9-3\"]],\n",
    "                             ('aunz', '2017'):[[\"Sep2016_table8-7\", \"Sep2017_table8-13\", \"Sep2017_table8-16\", \"Feb2018_table8-3\"], [\"Sep2016_table8-7\", \"Sep2017_table8-13\", \"Sep2017_table8-16\"], \n",
    "                                               [\"Sep2016_table8-7\", \"Sep2017_table8-13\", \"Sep2017_table8-12\", \"Feb2018_table8-3\"], [\"Sep2016_table8-7\", \"Sep2017_table8-13\", \"Feb2018_table8-3\"]],\n",
    "                             ('aunz', '2018'):[[\"Sep2018_table8-5\",\"Sep2017_table8-16\", \"Sep2017_table8-20\"], [\"Sep2018_table8-5\",\"Feb2018_table8-9\",\"Sep2017_table8-16\"]],\n",
    "                             ('aunz', '2019'):[[\"Feb2020_table7-11\", \"Sep2019_table8-3\", \"Feb2019_table8-9\", \"Feb2018_table8-9\", \"Feb2017_table8-5\", \"Sep2017_table21\",\"Sep2017_table8-16\"]],\n",
    "                             ('aunz', '2020'):[[\"Feb2021_table7-2\",\"Sep2019_table8-1\",\"Sep2020_table7-2\", \"Sep2019_table8-18\"],[\"Feb2021_table7-2\", \"Sep2019_table8-14\", \"Sep2020_table7-1\",\"Feb2019_table8-11\"],\n",
    "                                               [\"Sep2020_table7-7\",\"Sep2019_table8-19\",\"Sep2019_table8-18\", \"Sep2019_table8-16\"]],\n",
    "                             ('aunz', '2021'):[['Feb2021_table7-2', 'Sep2021_table7-4', 'Feb2022_table9-1'], ['Feb2021_table7-2', 'Sep2021_table7-7', 'Feb2022_table9-1'], \n",
    "                                               ['Feb2021_table7-2', 'Sep2021_table7-7', 'Sep2022_table10-6', 'Sep2022_table10-20'],\n",
    "                                               ['Feb2021_table7-2', 'Sep2021_table7-7', 'Sep2021_table7-1', 'Sep2022_table10-6', 'Sep2022_table10-20', 'Sep2019_table8-16']],\n",
    "                             ('aunz', '2022'):[[\"Sep2021_table7-6\", \"Feb2022_table9-1\", \"Feb2022_table9-10\", \"Sep2022_table10-18\"], [\"Sep2021_table7-6\", \"Feb2022_table9-1\", \"Feb2022_table9-2\",\"Sep2022_table10-18\"],\n",
    "                                               [\"Sep2021_table7-6\", \"Sep2022_table10-20\", \"Feb2022_table9-1\",\"Sep2022_table10-18\"]],\n",
    "                             ('aunz', '2023'):[['Feb2023_tableH3-10', 'Sep2023_tableH3-12', \"Sep2022_table10-20\", 'Sep2022_table10-12'],\n",
    "                                               ['Feb2023_tableH3-10', 'Sep2023_tableH3-12', 'Sep2023_tableH3-1', 'Sep2022_table10-25', 'Sep2021_table7-7'],\n",
    "                                               ['Feb2023_tableH3-10', 'Sep2023_tableH3-12', 'Sep2022_table10-12', 'Feb2022_table9-1'],\n",
    "                                               ['Feb2023_tableH3-10', 'Sep2023_tableH3-12', 'Sep2022_table10-12', 'Sep2022_table10-10', 'Feb2022_table9-1']],\n",
    "                            ('europe', '2002-2003'):[[\"2003_table5\", \"2003_table6\"], [\"2003_table5\"], [\"2003_table6\"]],\n",
    "                            ('europe', '2003-2004'):[[\"2003_table5\", \"2003_table6\"], [\"2003_table5\"], [\"2003_table6\"], [\"2004_table5\", \"2003_table6\"], [\"2004_table5\"]],\n",
    "                            ('europe', '2004-2005'):[[]],\n",
    "                            ('europe', '2005-2006'):[[]],\n",
    "                            ('europe', '2006-2007'):[[\"Sep2006_table3\", \"Sep2006_table6\"], [\"Sep2006_table5\", \"Sep2006_table3\"]],\n",
    "                            ('europe', '2007-2008'):[[\"Mar2007_table4\", \"Sep2006_table6\"], [\"Mar2007_table4\", \"Sep2006_table6\",\"Sep2007_table6\"], [\"Sep2007_table5\", \"Sep2007_table6\", \"Sep2007_table8\"],\n",
    "                                                     [\"Sep2007_table5\", \"Sep2007_table6\", \"Sep2007_table8\",\"Mar2007_table4\", \"Sep2006_table6\"]],\n",
    "                            ('europe', '2008-2009'):[[\"Sep2008_table4A\", \"Sep2007_table8\", \"Sep2008_table4B\"], [\"Sep2008_table4A\", \"Sep2007_table8\"], \n",
    "                                                     [\"Feb2009_table4\"], [\"Feb2009_table5\"], [\"Sep2009_table6\"]],\n",
    "                            ('europe', '2009-2010'):[[]],\n",
    "                            ('europe', '2010-2011'):[[]],\n",
    "                            ('europe', '2011-2012'):[[\"Sep2013_table7-12\", \"Sep2012_table9\"],[\"Sep2013_table7-12\", \"Sep2012_table10\"], [\"Sep2013_table7-12\", \"Feb2012_table12\"], \n",
    "                                                     [\"Sep2013_table7-12\", \"Feb2013_table13\"],[\"Sep2013_table7-12\", \"Sep2012_table15\"]],\n",
    "                            ('europe', '2012-2013'):[['Feb2012_table13', \"Feb2013_table18\", \"Feb2013_table13\"], ['Feb2012_table8', \"Feb2013_table18\", \"Sep2013_table7-16\"],\n",
    "                                                      [\"Sep2012_table18\", \"Sep2013_table7-7\"], [\"Sep2013_table7-8\", \"Sep2012_table17\"], [\"Sep2013_table7-3\", \"Sep2012_table9\", \"Sep2013_table7-16\"]],\n",
    "                            ('europe', '2013-2014'):[[\"Sep2012_table9\",\"Sep2013_table7-9\", \"Sep2013_table7-7\"],[\"Sep2012_table9\", \"Sep2013_table7-12\"],[\"Sep2012_table15\",\"Feb2013_table18\"],\n",
    "                                                     [\"Sep2012_table9\", \"Feb2014_table9-7\", \"Sep2013_table7-10\"], [\"Sep2012_table15\", 'Sep2013_table7-10', \"Feb2014_table9-7\"]], \n",
    "                            ('europe', '2014-2015'):[[\"Sep2012_table15\", \"Sep2013_table7-7\", \"Sep2014_table8-19\", \"Feb2014_table9-7\", \"Sep2014_table8-11\"], \n",
    "                                                      [\"Sep2015_table8-9\", \"Sep2014_table8-16\", \"Sep2014_table8-10\", 'Feb2014_table9-2', \"Feb2015_table9-4\", \"Sep2012_table12\", \"Sep2012_table15\"]],\n",
    "                            ('europe', '2015-2016'):[[\"Sep2016_table8-6\",\"Feb2016_table9-2\"],[\"Sep2015_table8-4\",\"Sep2015_table8-13\",\"Sep2015_table8-6\"],\n",
    "                                                     [\"Sep2015_table8-8\",\"Sep2015_table8-9\"],[\"Sep2015_table8-13\",\"Sep2015_table8-9\"],[\"Feb2016_table9-3\"]] ,\n",
    "                            ('europe', '2016-2017'):[[\"Sep2016_table8-6\", \"Sep2016_table8-7\"], [\"Sep2016_table8-7\", \"Feb2016_table9-3\"], [\"Sep2016_table8-7\", \"Feb2016_table9-3\", \"Sep2016_table8-2\"],\n",
    "                                                      [\"Sep2016_table8-7\", \"Sep2016_table8-2\"]],\n",
    "                            ('europe', '2017-2018'):[[\"Sep2016_table8-7\", \"Sep2017_table21\"], [\"Sep2016_table8-7\", \"Feb2017_table8-2\"], [\"Sep2016_table8-7\", \"Sep2016_table8-9\", \"Sep2017_table8-2\"],\n",
    "                                                      [\"Sep2016_table8-7\", \"Sep2016_table8-9\", \"Feb2017_table8-2\"], [\"Sep2016_table8-7\", \"Feb2017_table8-2\", \"Sep2017_table21\"]],\n",
    "                            ('europe', '2018-2019'):[[\"Feb2019_table8-5\",\"Feb2018_table8-4\",\"Sep2017_table8-16\"],[\"Sep2018_table8-6\", \"Sep2018_table8-5\",\"Feb2018_table8-6\"], [\"Sep2018_table8-6\",\"Feb2018_table8-6\",\"Sep2017_table21\"],\n",
    "                                                     [\"Feb2018_table8-6\",\"Sep2017_table21\", \"Sep2018_table8-7\"], [\"Sep2018_table8-7\", \"Sep2018_table8-2\", \"Sep2018_table8-5\"]],\n",
    "                            ('europe', '2019-2020'):[[\"Feb2020_table7-12\", \"Sep2019_table8-15\", \"Sep2019_table8-6\"], [\"Sep2019_table8-7\", \"Sep2019_table8-17\", \"Feb2019_table8-10\"],\n",
    "                                                      [\"Sep2019_table8-13\", \"Sep2019_table8-14\", \"Sep2019_table8-15\"], [\"Sep2019_table8-17\", \"Sep2019_table8-18\"],\n",
    "                                                      [\"Sep2019_table8-6\", \"Sep2019_table8-3\", \"Sep2019_table8-5\"]],                                                                                                                                             \n",
    "                            ('europe', '2020-2021'):[[\"Feb2020_table7-5\", \"Feb2021_table7-3\",\"Feb2020_table7-12\"], [\"Feb2020_table7-5\", \"Sep2019_table8-16\", \"Sep2021_table7-4\"], \n",
    "                                                      [\"Feb2020_table7-5\",\"Sep2020_table7-9\",\"Feb2022_table9-1\",\"Sep2021_table7-4\"], [\"Feb2020_table7-5\", \"Sep2020_table7-1\", \"Sep2021_table7-5\"]],\n",
    "                            ('europe', '2021-2022'):[[\"Feb2020_table7-5\", \"Feb2021_table7-2\",\"Sep2021_table7-10\",\"Sep2022_table10-17\"],[\"Feb2020_table7-5\", \"Feb2021_table7-2\",\"Sep2021_table7-10\",\"Feb2022_table9-9\"],\n",
    "                                                     [\"Feb2020_table7-5\", \"Feb2021_table7-2\",\"Sep2021_table7-10\",\"Feb2022_table9-7\"],[\"Feb2020_table7-5\", \"Feb2021_table7-2\",\"Feb2021_table7-3\",\"Sep2021_table7-10\"]],\n",
    "                            ('europe', '2022-2023'):[[\"Sep2021_table7-6\",\"Feb2022_table9-11\",\"Sep2022_table10-8\"],[\"Sep2021_table7-6\",\"Sep2022_table10-8\",\"Feb2023_tableH3-10\",\"Feb2023_tableH3-1\"],\n",
    "                                                     [\"Sep2021_table7-6\",\"Sep2022_table10-19\"],[\"Sep2021_table7-6\",\"Sep2022_table10-18\"],[\"Sep2021_table7-6\",\"Sep2022_table10-16\",\"Feb2023_tableH3-1\",\"Feb2023_tableH3-10\"]],\n",
    "                            ('us', '2002-2003'):[[\"2003_table5\", \"2003_table6\"], [\"2003_table5\"], [\"2003_table6\"]],\n",
    "                            ('us', '2003-2004'):[[\"2003_table5\", \"2003_table6\"], [\"2003_table5\"], [\"2003_table6\"], [\"2004_table5\", \"2003_table6\"], [\"2004_table5\"]],\n",
    "                            ('us', '2004-2005'):[[]],\n",
    "                            ('us', '2005-2006'):[[]],\n",
    "                            ('us', '2006-2007'):[[]],                          \n",
    "                            ('us', '2007-2008'):[[\"Mar2006_table4\", \"Sep2006_table4\",'Sep2006_table6','Sep2007_table6',\"Sep2008_table4A\",\"Sep2007_table7\"]],  \n",
    "                            ('us', '2008-2009'):[[\"Feb2009_table5\"], [\"Sep2009_table6\"], [\"Feb2009_table4\"],[\"Sep2008_table4A\", \"Sep2007_table8\", 'Sep2008_table4B'], \n",
    "                                                 [\"Sep2008_table4A\", \"Sep2007_table8\"]],\n",
    "                            ('us', '2009-2010'):[['Sep2009_table6',\"Feb2010_table12\"],[\"Sep2009_table6\"]],\n",
    "                            ('us', '2010-2011'):[[]],\n",
    "                            ('us', '2011-2012'):[[\"Sep2013_table7-12\",\"Feb2013_table13\"],[\"Sep2013_table7-12\",\"Feb2012_table14\"],[\"Sep2013_table7-12\",\"Feb2012_table12\"],\n",
    "                                                 [\"Sep2013_table7-12\",\"Sep2012_table15\"],[\"Sep2013_table7-12\",\"Sep2012_table9\"]],\n",
    "                            ('us', '2012-2013'):[[\"Sep2013_table7-10\", \"Feb2012_table12\"], [\"Sep2013_table7-10\", \"Feb2012_table13\"], [\"Sep2013_table7-10\", \"Feb2012_table14\"],\n",
    "                                                  [\"Sep2013_table7-10\", \"Feb2012_table11\"], [\"Sep2013_table7-10\", \"Sep2012_table12\"]],\n",
    "                            ('us', '2013-2014'):[['Sep2012_table15', 'Sep2013_table7-10', 'Feb2014_table9-2', 'Sep2014_table8-10', \"Sep2014_table8-19\"],\n",
    "                                                  ['Sep2012_table9', 'Sep2013_table7-10', 'Feb2014_table9-7', 'Sep2014_table8-13', 'Feb2015_table9-3', 'Feb2015_table9-4']],\n",
    "                            ('us', '2014-2015'):[[\"Sep2012_table9\", \"Sep2013_table7-7\", 'Feb2014_table9-7', 'Sep2014_table8-10', \"Sep2014_table8-16\", 'Feb2015_table9-4',  \"Sep2015_table8-9\"],\n",
    "                                                  [\"Sep2013_table7-8\", \"Sep2012_table15\", \"Sep2014_table8-19\", \"Sep2014_table8-10\", \"Feb2014_table9-6\"],\n",
    "                                                  [\"Sep2013_table7-8\", \"Feb2015_table9-4\", \"Sep2012_table9\", \"Sep2014_table8-19\", \"Sep2014_table8-10\", \"Feb2014_table9-6\"]],\n",
    "                            ('us', '2015-2016'):[[\"Feb2016_table9-3\"], [\"Sep2014_table8-19\",\"Feb2015_table9-2\"], [\"Feb2015_table9-4\",\"Feb2015_table9-2\"],\n",
    "                                                 [\"Sep2015_table8-9\",\"Sep2015_table8-8\"], [\"Sep2016_table8-6\",\"Sep2016_table8-2\"]],\n",
    "                            ('us', '2016-2017'):[['Sep2016_table8-7', 'Sep2016_table8-6'], ['Sep2016_table8-7', 'Sep2016_table8-2'], \n",
    "                                                  ['Sep2016_table8-7','Feb2016_table9-3'], ['Sep2016_table8-7',  'Sep2016_table8-2', 'Feb2016_table9-3']],\n",
    "                            ('us', '2017-2018'):[['Sep2016_table8-7', 'Sep2017_table8-16', 'Sep2017_table8-2'], ['Sep2016_table8-7', 'Sep2017_table8-2'],\n",
    "                                                  ['Sep2016_table8-7', 'Feb2017_table8-2']],\n",
    "                            ('us', '2018-2019'):[['Sep2017_table8-16','Feb2018_table8-6','Feb2019_table8-7'], ['Feb2019_table8-9', 'Feb2018_table8-9','Sep2017_table8-2'], \n",
    "                                                 ['Feb2018_table8-9', 'Feb2019_table8-7', 'Sep2017_table21']],\n",
    "                            ('us', '2019-2020'):[[\"Sep2019_table8-1\",\"Sep2019_table8-11\",\"Sep2019_table8-14\"],[\"Sep2019_table8-6\",\"Sep2019_table8-3\",\"Sep2019_table8-1\"],\n",
    "                                                 [\"Feb2019_table8-11\",\"Sep2019_table8-14\",\"Sep2019_table8-15\"],[\"Sep2019_table8-1\",\"Sep2019_table8-18\"]],\n",
    "                            ('us', '2020-2021'):[[\"Sep2021_table7-9\",\"Sep2020_table7-5\"],[\"Sep2020_table7-9\", \"Sep2021_table7-4\"], \n",
    "                                                 [\"Feb2021_table7-3\",\"Feb2020_table7-12\"],[\"Feb2022_table9-6\",\"Feb2022_table9-8\"],[\"Sep2021_table7-4\",\"Sep2020_table7-5\"]],\n",
    "                            ('us', '2021-2022'):[['Feb2021_table7-2','Feb2020_table7-5','Sep2022_table10-18', 'Sep2021_table7-4']],                                                                                                       \n",
    "                            ('us', '2022-2023'):[[\"Sep2021_table7-6\",\"Sep2022_table10-19\"],[\"Sep2021_table7-6\",\"Sep2022_table10-17\",\"Sep2022_table10-16\"],\n",
    "                                                 [\"Sep2021_table7-6\",\"Feb2023_tableH3-10\",\"Feb2023_tableH3-1\"]],\n",
    "                             \n",
    "                             }\n",
    "\n",
    "manual_HI_table_selection = [list(k)+[\", \".join(l)] for k,v in manual_HI_table_selection.items() for l in v]\n",
    "manual_HI_table_selection = pd.DataFrame.from_records(manual_HI_table_selection, columns=[\"region\", \"season\", \"table list\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (region, season) in closest_HI_strains.index.unique():\n",
    "    if (region, season) in manual_HI_table_selection.set_index([\"region\", \"season\"]).sort_index().index.to_list():\n",
    "        continue\n",
    "    print (f\"('{region}', '{season}')\")\n",
    "\n",
    "    sdf = closest_HI_strains.loc[(region, season), :].set_index(\"strain\")\n",
    "    HI_antigen_strains = [s for sl in sdf[\"HI antigens\"].tolist() for s in sl.split(\", \")]\n",
    "    HI_antigen_tables = HI_titers[HI_titers[\"strain\"].isin(HI_antigen_strains)][[\"table\", \"type\", \"strain\"]].drop_duplicates(ignore_index=True)\n",
    "\n",
    "    for t in HI_antigen_tables[\"type\"].unique():\n",
    "        tdf = HI_antigen_tables[HI_antigen_tables[\"type\"]==t]\n",
    "\n",
    "        #determine antigens for each strain\n",
    "        ds = tdf[tdf[\"strain\"].isin(sdf.loc[\"dominant strain\", \"HI antigens\"].split(\", \"))][\"strain\"].unique().tolist()\n",
    "        \n",
    "        vs = tdf[tdf[\"strain\"].isin(sdf.loc[\"vaccine strain\", \"HI antigens\"].split(\", \"))][\"strain\"].unique().tolist()\n",
    "        css = tdf[tdf[\"strain\"].isin(sdf.loc[\"reproducible selection at WHO-timing\", \"HI antigens\"].split(\", \"))][\"strain\"].unique().tolist()\n",
    "        lss = tdf[tdf[\"strain\"].isin(sdf.loc[\"reproducible selection at delayed timing\", \"HI antigens\"].split(\", \"))][\"strain\"].unique().tolist()\n",
    "\n",
    "        #if no antigens for any of the strains continue\n",
    "        if len(ds)==0 or len(vs)==0 or len(css)==0 or len(lss)==0:\n",
    "            continue \n",
    "\n",
    "        #for each strain determine in which table they are\n",
    "        ds_tables, vs_tables, css_tables, lss_tables = {}, {}, {},{}\n",
    "        for tb in tdf[\"table\"].unique():\n",
    "            ts = tdf[tdf[\"table\"]==tb][\"strain\"].unique().tolist()\n",
    "        \n",
    "            ds_tables[tb] = [s for s in ts if s in  ds]\n",
    "            vs_tables[tb] = [s for s in ts if s in  vs]\n",
    "            css_tables[tb] = [s for s in ts if s in css]\n",
    "            lss_tables[tb] = [s for s in ts if s in lss]\n",
    "\n",
    "        #remove empty tables\n",
    "        ds_tables = {k:v for k, v in ds_tables.items() if len(v)>0}\n",
    "        vs_tables = {k:v for k, v in vs_tables.items() if len(v)>0}\n",
    "        css_tables = {k:v for k, v in css_tables.items() if len(v)>0}\n",
    "        lss_tables = {k:v for k, v in lss_tables.items() if len(v)>0}\n",
    "\n",
    "        #make an overview of the tables and the number of strain is each table\n",
    "        table_overview = []\n",
    "        for i, tb in enumerate(set(list(ds_tables.keys()) + list(vs_tables.keys()) + list(css_tables.keys())+list(lss_tables.keys())), start=1):\n",
    "            l = []\n",
    "            l.append(len(ds_tables[tb]) if tb in ds_tables else 0)\n",
    "            l.append(len(vs_tables[tb]) if tb in vs_tables else 0)\n",
    "            l.append(len(css_tables[tb]) if tb in css_tables else 0)\n",
    "            l.append(len(lss_tables[tb]) if tb in lss_tables else 0)\n",
    "            table_overview.append([i, tb]+l)\n",
    "\n",
    "        table_overview = pd.DataFrame.from_records(table_overview, columns=[\"tn\", \"table\", \"ds\", \"vs\", \"css\", \"lss\"])\n",
    "\n",
    "        print (f\"ds: {len(ds)}, vs: {len(vs)}, css: {len(css)}, lss: {len(lss)} \")\n",
    "        table_overview = table_overview.set_index(\"table\").sort_index()\n",
    "        table_overview.index.name = None\n",
    "        print(table_overview[[\"ds\", \"vs\", \"css\", \"lss\"]].to_string())\n",
    "\n",
    "                \n",
    "        ds_, vs_, css_, lss_= ds, vs, css,lss\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "csl = [[]]\n",
    "\n",
    "for cs in csl:\n",
    "    for i in cs:\n",
    "        print (HI_table_merge[i])\n",
    "    try:     \n",
    "        print (can_merge_tables(cs, HI_table_merge), len(cs))\n",
    "        print (count_items_in_tables(cs, [ds_, vs_, css_, lss_], [ds_tables, vs_tables, css_tables, lss_tables]))\n",
    "    except:\n",
    "        pass\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign map code to keep track of the maps\n",
    "mc_count = 0\n",
    "map_codes = {}\n",
    "for tl in manual_HI_table_selection[\"table list\"].unique():\n",
    "    if len(tl)== 0 or pd.isna(tl):\n",
    "        continue\n",
    "    mc_count += 1\n",
    "    map_codes[tl] = mc_count\n",
    "\n",
    "#annotated in df\n",
    "for i, row in manual_HI_table_selection.iterrows():\n",
    "    if len(row[\"table list\"]) > 0 and not pd.isna(row[\"table list\"]): \n",
    "        manual_HI_table_selection.loc[i, \"map code\"] = map_codes[row[\"table list\"]]\n",
    "     \n",
    "manual_HI_table_selection = manual_HI_table_selection.set_index([\"region\", \"season\"]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Determine HI antigens for each region, season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>strain</th>\n",
       "      <th>HI antigens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">aunz</th>\n",
       "      <th>2003</th>\n",
       "      <td>dominant strain</td>\n",
       "      <td>A/Fujian/411/02, A/Finland/170/03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>vaccine strain</td>\n",
       "      <td>A/Panama/2007/99, A/Panama/2007/1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>reproducible selection at WHO-timing</td>\n",
       "      <td>A/New York/55/01, A/Latvia/1506/03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>reproducible selection at delayed timing</td>\n",
       "      <td>A/New York/55/01, A/Latvia/1506/03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>dominant strain</td>\n",
       "      <td>A/Stockholm/15/2004, A/Norway/70/2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">us</th>\n",
       "      <th>2021-2022</th>\n",
       "      <td>reproducible selection at delayed timing</td>\n",
       "      <td>A/Qatar/16-VI-19-0049409/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-2023</th>\n",
       "      <td>dominant strain</td>\n",
       "      <td>A/Netherlands/10205/2021, A/Netherlands/10884/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-2023</th>\n",
       "      <td>vaccine strain</td>\n",
       "      <td>A/Darwin/9/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-2023</th>\n",
       "      <td>reproducible selection at WHO-timing</td>\n",
       "      <td>A/Darwin/6/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-2023</th>\n",
       "      <td>reproducible selection at delayed timing</td>\n",
       "      <td>A/Netherlands/00007/2021, A/Beirut/5518/2021, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    strain  \\\n",
       "region season                                                \n",
       "aunz   2003                                dominant strain   \n",
       "       2003                                 vaccine strain   \n",
       "       2003           reproducible selection at WHO-timing   \n",
       "       2003       reproducible selection at delayed timing   \n",
       "       2004                                dominant strain   \n",
       "...                                                    ...   \n",
       "us     2021-2022  reproducible selection at delayed timing   \n",
       "       2022-2023                           dominant strain   \n",
       "       2022-2023                            vaccine strain   \n",
       "       2022-2023      reproducible selection at WHO-timing   \n",
       "       2022-2023  reproducible selection at delayed timing   \n",
       "\n",
       "                                                        HI antigens  \n",
       "region season                                                        \n",
       "aunz   2003                       A/Fujian/411/02, A/Finland/170/03  \n",
       "       2003                    A/Panama/2007/99, A/Panama/2007/1999  \n",
       "       2003                      A/New York/55/01, A/Latvia/1506/03  \n",
       "       2003                      A/New York/55/01, A/Latvia/1506/03  \n",
       "       2004                   A/Stockholm/15/2004, A/Norway/70/2005  \n",
       "...                                                             ...  \n",
       "us     2021-2022                      A/Qatar/16-VI-19-0049409/2019  \n",
       "       2022-2023  A/Netherlands/10205/2021, A/Netherlands/10884/...  \n",
       "       2022-2023                                    A/Darwin/9/2021  \n",
       "       2022-2023                                    A/Darwin/6/2021  \n",
       "       2022-2023  A/Netherlands/00007/2021, A/Beirut/5518/2021, ...  \n",
       "\n",
       "[252 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_HI_strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_HI_antigens = []\n",
    "for region, h in region_hemispheres.items():\n",
    "    for season, (fss, fse) in flu_seasons[h].items():\n",
    "\n",
    "        df = closest_HI_strains.loc[(region, season),:]\n",
    "        tdf = manual_HI_table_selection.loc[(region, season),:]\n",
    "        \n",
    "        for i, row in tdf.iterrows():\n",
    "            tables = row[\"table list\"].split(\", \")\n",
    "            tables_antigen_stains = HI_titers[HI_titers[\"table\"].isin(tables)][\"strain\"].unique().tolist()\n",
    "            \n",
    "            for j, row2 in df.iterrows():\n",
    "                strain = row2[\"strain\"]\n",
    "                possible_antigens = row2[\"HI antigens\"].split(\", \")\n",
    "\n",
    "                selected_ag = [ag for ag in possible_antigens if ag in tables_antigen_stains]\n",
    "\n",
    "                selected_HI_antigens.append([region, season, row[\"map code\"], row[\"table list\"], strain, \", \".join(selected_ag)])\n",
    "\n",
    "selected_HI_antigens = pd.DataFrame.from_records(selected_HI_antigens, columns=[\"region\", \"season\", \"map code\", \"table list\", \"strain\", \"selected antigens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_HI_antigens_overview = pd.pivot(selected_HI_antigens, index=[\"region\", \"season\", \"map code\"], columns=[\"strain\"], values=[\"selected antigens\"])\n",
    "selected_HI_antigens_overview.columns = selected_HI_antigens_overview.columns.droplevel()\n",
    "selected_HI_antigens_overview.columns.name = None\n",
    "selected_HI_antigens_overview = selected_HI_antigens_overview[[\"dominant strain\", \"vaccine strain\", \"reproducible selection at WHO-timing\", \"reproducible selection at delayed timing\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_HI_antigens_overview.reset_index().to_csv(\"../analysis/antigenic_comparison/selected_antigens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 get actual amino acid difference of between the selected antigens and the actual strains\n",
    "\n",
    "also gettig the GISAID IDs of the selected HI antigens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    selected_HI_antigens = selected_HI_antigens.set_index([\"region\", \"season\", \"map code\"]).sort_index()\n",
    "except:\n",
    "    selected_HI_antigens = selected_HI_antigens.reset_index().set_index([\"region\", \"season\", \"map code\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "antigen_strain_ids = {}\n",
    "strain_HI_antigen_mutations = []\n",
    "\n",
    "seen = []\n",
    "for (region, season, mapcode), row in selected_HI_antigens.iterrows():\n",
    "    if pd.isna(row[\"selected antigens\"]) or row[\"selected antigens\"] == \"\":\n",
    "        continue\n",
    "    \n",
    "    strain= row[\"strain\"]\n",
    "    if strain == \"dominant\":\n",
    "        seq = dominant_strains.loc[(region, season), \"sequence\"]\n",
    "    elif strain == \"vaccine strain\":\n",
    "        seq = vaccine_strains[vaccine_strain_names[region_hemispheres[region]][season]][\"aa\"][\"HA1\" if season in early_seasons else \"complete\"]\n",
    "    elif strain == \"reproducible selection at WHO-timing\":\n",
    "        seq = reproducible_selection_strains.loc[(region, season, \"WHO-timing\"),\"sequence\"]\n",
    "    else:\n",
    "        seq = reproducible_selection_strains.loc[(region, season, \"delayed-timing\"),\"sequence\"]\n",
    "\n",
    "    antigens = row[\"selected antigens\"].split(\", \")\n",
    "    for antigen in antigens:\n",
    "        if antigen not in antigen_strain_ids.keys():\n",
    "            antigen_strain_ids[antigen] = tested_antigen_ids[antigen]\n",
    "        \n",
    "        if (region, season, strain, antigen) not in seen:\n",
    "            if season in early_seasons:\n",
    "                antigen_seq = HA1_HI_antigen_seqs[antigen]\n",
    "            else:\n",
    "                antigen_seq = HI_antigen_seqs[antigen]\n",
    "\n",
    "            muts = get_mutations(seq, antigen_seq, sl=\"HA1\" if season in early_seasons else \"complete\")\n",
    "            muts[[\"region\", \"season\", \"strain\", \"antigen\"]] = [region, season, strain, antigen]\n",
    "            try:\n",
    "                strain_HI_antigen_mutations = pd.concat([strain_HI_antigen_mutations, muts], ignore_index=True)\n",
    "            except:\n",
    "                strain_HI_antigen_mutations = muts\n",
    "            seen.append((region, season, strain, antigen))\n",
    "\n",
    "antigen_strain_ids = pd.DataFrame.from_dict(antigen_strain_ids, orient=\"index\", columns=[\"GISAID ID\"]).reset_index().rename(columns={\"index\":\"HI antigen strain\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_HI_antigen_mutations.to_csv(\"../analysis/antigenic_comparison/mutations_between_selected_HI_antigens_and_strain.csv\", index=False)\n",
    "antigen_strain_ids.to_csv(\"../analysis/antigenic_comparison/GISAID_ids_of_HI_antigens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Prep R-script for antigenic cartography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine all unique table maps to be loaded \n",
    "individual_tables = []\n",
    "for tl in map_codes.keys():\n",
    "    for t in tl.split(\", \"):\n",
    "        if t not in individual_tables:\n",
    "            individual_tables.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1. antigenic maps construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to write the antigenic maps\n",
    "with open(\"../scripts/construct_antigenic_maps.R\", \"w\") as fw:\n",
    "    fw.write(\"library(Racmacs)\\n\")\n",
    "\n",
    "    fw.write(\"options(RacOptimizer.num_cores = 10)\\n\")\n",
    "    fw.write('dir <- \"./data/HI_data/individual_HI_tables\"\\n')\n",
    "    fw.write('acmap_dir <- \"./analysis/antigenic_comparison/antigenic_maps/complete_maps\"\\n')\n",
    "    fw.write('coords_dir <- \"./analysis/antigenic_comparison/antigenic_maps/coords\"\\n\\n')\n",
    "\n",
    "\n",
    "    for i in individual_tables:\n",
    "        fw.write (f\"t{i.replace('table', '').replace('-', '_')} <- read.titerTable(file.path(dir, '{i}.csv'))\\n\")\n",
    "\n",
    "    fw.write (\"\\n#load individual maps\\n\")\n",
    "    for i in individual_tables:\n",
    "        i = i.replace('table', '').replace('-', '_')\n",
    "        fw.write(f\"map{i} <- acmap( titer_table=t{i}, sr_names=colnames(t{i}), ag_names=rownames(t{i}))\\n\\n\")\n",
    "    \n",
    "\n",
    "    for tl, n in map_codes.items():\n",
    "        tl = tl.split(\", \")\n",
    "\n",
    "        maplist = \", \".join([f\"map{i.replace('table', '').replace('-', '_')}\" for i in tl])\n",
    "\n",
    "        fw.write(f\"map{n} <- mergeMaps({maplist}, method='table', number_of_dimensions = 2)\\n\" )\n",
    "        fw.write(f\"map{n} <- optimizeMap(map = map{n}, number_of_dimensions = 2, number_of_optimizations = {(round(1000*len(tl)*3))}, options = list(ignore_disconnected = TRUE))\\n\")\n",
    "        #print(f\"map{n} <- keepBestOptimization(map{n})\")\n",
    "        fw.write(f\"save.acmap(map{n}, file.path(acmap_dir, 'map{n}.ace'))\\n\")\n",
    "        fw.write(f\"save.coords(map{n}, file.path(coords_dir, 'map{n}.csv'))\\n\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. antigenic map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../scripts/summarize_antigenic_maps.R\", \"w\") as fw:\n",
    "    \n",
    "    fw.write(\"library(Racmacs)\\n\")\n",
    "    fw.write(\"library(ggplot2)\\n\")\n",
    "    fw.write(\"options(RacOptimizer.num_cores = 10)\\n\")\n",
    "    \n",
    "    fw.write('dir <- \"./data/HI_data/individual_HI_tables/\"\\n')\n",
    "    fw.write('acmap_dir <- \"./analysis/antigenic_comparison/antigenic_maps/complete_maps\"\\n')\n",
    "    fw.write('acmap_simple_dir <- \"./analysis/antigenic_comparison/antigenic_maps/maps\"\\n')\n",
    "    fw.write('coords_dir <- \"./analysis/antigenic_comparison/antigenic_maps/coords\"\\n')\n",
    "\n",
    "    fw.write(\"#load individual maps\\n\")\n",
    "    for mapcode in map_codes.values():\n",
    "        fw.write(f\"map{mapcode} <- keepBestOptimization(read.acmap(file.path(acmap_dir, 'map{mapcode}.ace')))\\n\")\n",
    "        fw.write(f\"save.acmap(map{mapcode}, file.path(acmap_simple_dir, 'map{mapcode}.ace'))\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_colors = [\"#ea5545\",\"#ef9b20\",\"#ede15b\",\"#87bc45\",\"#27aeef\",\"#b33dc6\", \"#4e00ff\"]\n",
    "ag_colors = {\"shared\":\"#000000\", #black\n",
    "             \"ds_vs_css\":\"#6e6c02\", #mustard\n",
    "             \"ds_vs_lss\":\"#093303\", #dark green\n",
    "             \"ds_css_lss\":\"#00c1f3\", #cyan\n",
    "             \"vs_css_lss\":\"#d399d8\", #light pink\n",
    "             \"ds_vs\":\"#331d03\", #brown\n",
    "             \"ds_css\":\"#584e82\", #mauva\n",
    "             \"ds_lss\":\"#9fd600\", #green\n",
    "             \"vs_css\":\"#cf0030\", #red\n",
    "             \"vs_lss\":\"#7eb4f2\", #baby blue\n",
    "             \"css_lss\":\"#6d21b8\", #purple\n",
    "             \"ds\":\"#fcd74e\", #yelllow\n",
    "             \"vs\":\"#ffa600\", #orange\n",
    "             \"css\":\"#d900ba\", #pink\n",
    "             \"lss\":\"#0051ff\"} #darkblue\n",
    "\n",
    "color_seasons = True\n",
    "\n",
    "with open(\"../scripts/visualize_antigenic_maps.R\", \"w\") as fw:\n",
    "    \n",
    "    fw.write(\"library(Racmacs)\\n\")\n",
    "    fw.write(\"library(ggplot2)\\n\")\n",
    "    fw.write(\"options(RacOptimizer.num_cores = 10)\\n\\n\")\n",
    "    \n",
    "    fw.write(\"setwd('~/Desktop/later-strain-selection')\\n\\n\")\n",
    "\n",
    "    fw.write('dir <- \"./data/HI_data/individual_HI_tables/\"\\n')\n",
    "    fw.write('acmap_dir <- \"./analysis/antigenic_comparison/antigenic_maps/maps\"\\n')\n",
    "    fw.write('coords_dir <- \"./analysis/antigenic_comparison/antigenic_maps/coords\"\\n')\n",
    "    fw.write('table_fig_dir <- \"./figures/antigenic_maps/colored_by_table\"\\n')\n",
    "    fw.write('strain_fig_dir <- \"./figures/antigenic_maps/colored_by_strain\"\\n\\n')\n",
    "\n",
    "    fw.write(\"#load individual maps\\n\")\n",
    "    for mapcode in map_codes.values():\n",
    "        fw.write(f\"map{mapcode} <- read.acmap(file.path(acmap_dir, 'map{mapcode}.ace'))\\n\")\n",
    "        \n",
    "    #check quality of maps\n",
    "    fw.write (\"\\n# check if maps are good\")\n",
    "    for mapcode in map_codes.values():\n",
    "        fw.write(f\"\\ncheckHemisphering(map{mapcode})\")\n",
    "    \n",
    "    for tl, mapcode in map_codes.items():\n",
    "\n",
    "        fw.write(f\"\\n#################### MAP {mapcode} ####################\\n\")\n",
    "        fw.write(\"#### colered by tables\\n\")\n",
    "        #start with all antigens grey\n",
    "        fw.write(f\"agSize(map{mapcode}) <- 5\\n\")\n",
    "        fw.write(f\"agFill(map{mapcode}) <- 'grey50'\\n\")\n",
    "\n",
    "        #assign antigens\n",
    "        tl = tl.split(\", \")\n",
    "        if len(tl) >1:\n",
    "            for i, table in enumerate(tl):\n",
    "                table_antigens = HI_titers[HI_titers[\"table\"]==table][\"antigen\"].unique().tolist()\n",
    "                non_table_antigens = HI_titers[HI_titers[\"table\"].isin([t for t in tl if t!=table])][\"antigen\"].unique().tolist()\n",
    "                table = table.replace(\"-\",\"_\")\n",
    "\n",
    "                unique_table_antigens = [ta for ta in table_antigens if ta not in non_table_antigens]\n",
    "                write_antigens(fw, mapcode, unique_table_antigens, f\"ag_{table}\")\n",
    "            \n",
    "            #color antigens\n",
    "            fw.write(\"\\n\")\n",
    "            for i, table in enumerate(tl):\n",
    "                table = table.replace(\"-\",\"_\")\n",
    "\n",
    "                fw.write(f\"agFill(map{mapcode})[ag_{table}] <- '{table_colors[i]}'\\n\") \n",
    "\n",
    "        fw.write(f\"p_map{mapcode} <- ggplot(map{mapcode}) + ggtitle('map {mapcode} ({', '.join(tl)})')\\n\")\n",
    "        fw.write(f\"p_map{mapcode}_png <- file.path(table_fig_dir, 'map{mapcode}.png')\\n\")\n",
    "        fw.write(f\"png(file=p_map{mapcode}_png)\\n\")\n",
    "        fw.write(f\"p_map{mapcode}\\n\")\n",
    "        fw.write(\"dev.off()\\n\\n\")\n",
    "\n",
    "        if color_seasons:\n",
    "\n",
    "            fw.write(\"#### colered by region and season\\n\")\n",
    "\n",
    "            df = selected_HI_antigens.reset_index()\n",
    "            df = df[df[\"map code\"]==mapcode].set_index([\"region\", \"season\"]).sort_index()\n",
    "\n",
    "            for (region, season) in df.index.unique():\n",
    "                sdf = df.loc[(region, season), :].set_index([\"strain\"]).sort_index()\n",
    "                fw.write(f\"\\n####### {region}, {season} #########\\n\")\n",
    "                #start with all antigens grey\n",
    "                fw.write(f\"agSize(map{mapcode}) <- 5\\n\")\n",
    "                fw.write(f\"agFill(map{mapcode}) <- 'grey50'\\n\")\n",
    "\n",
    "                #get antigens of the strains\n",
    "                ds_ag = sdf.loc[\"dominant strain\", \"selected antigens\"].split(\", \")\n",
    "                ds_ag = HI_titers[(HI_titers[\"strain\"].isin(ds_ag))&(HI_titers[\"table\"].isin(tl))][\"antigen\"].unique().tolist()\n",
    "                vs_ag = sdf.loc[\"vaccine strain\", \"selected antigens\"].split(\", \")\n",
    "                vs_ag = HI_titers[(HI_titers[\"strain\"].isin(vs_ag))&(HI_titers[\"table\"].isin(tl))][\"antigen\"].unique().tolist()\n",
    "                vs_ag = [ag.replace(\",\", \"\") for ag in vs_ag]\n",
    "                css_ag = sdf.loc[\"reproducible selection at WHO-timing\", \"selected antigens\"].split(\", \")\n",
    "                css_ag = HI_titers[(HI_titers[\"strain\"].isin(css_ag))&(HI_titers[\"table\"].isin(tl))][\"antigen\"].unique().tolist()\n",
    "                lss_ag = sdf.loc[\"reproducible selection at delayed timing\", \"selected antigens\"].split(\", \")\n",
    "                lss_ag = HI_titers[(HI_titers[\"strain\"].isin(lss_ag))&(HI_titers[\"table\"].isin(tl))][\"antigen\"].unique().tolist()\n",
    "\n",
    "\n",
    "                #get possible combinations\n",
    "                shared_ag = [ag for ag in ds_ag if ag in vs_ag and ag in css_ag and ag in lss_ag]\n",
    "                ds_vs_css_ag = [ag for ag in ds_ag if ag in vs_ag and ag in css_ag and ag not in lss_ag]\n",
    "                ds_vs_lss_ag= [ag for ag in ds_ag if ag in vs_ag and ag not in css_ag and ag in lss_ag]\n",
    "                ds_css_lss_ag = [ag for ag in ds_ag if ag not in vs_ag and ag in css_ag and ag in lss_ag]\n",
    "                vs_css_lss_ag = [ag for ag in vs_ag if ag not in ds_ag and ag in css_ag and ag in lss_ag]\n",
    "                ds_vs_ag = [ag for ag in ds_ag if ag in vs_ag and ag not in css_ag and ag not in lss_ag]\n",
    "                ds_css_ag = [ag for ag in ds_ag if ag not in vs_ag and ag in css_ag and ag not in lss_ag]\n",
    "                ds_lss_ag = [ag for ag in ds_ag if ag not in vs_ag and ag not in css_ag and ag in lss_ag]\n",
    "                vs_css_ag = [ag for ag in vs_ag if ag not in ds_ag and ag in css_ag and ag not in lss_ag]\n",
    "                vs_lss_ag = [ag for ag in vs_ag if ag not in ds_ag and ag not in css_ag and ag in lss_ag]\n",
    "                css_lss_ag = [ag for ag in css_ag if ag not in ds_ag and ag not in vs_ag and ag in lss_ag]\n",
    "\n",
    "                ds_u_ag = [ag for ag in ds_ag if ag not in vs_ag and ag not in css_ag and ag not in lss_ag]\n",
    "                vs_u_ag = [ag for ag in vs_ag if ag not in ds_ag and ag not in css_ag and ag not in lss_ag]\n",
    "                css_u_ag = [ag for ag in css_ag if ag not in ds_ag and ag not in vs_ag and ag not in lss_ag]\n",
    "                lss_u_ag = [ag for ag in lss_ag if ag not in ds_ag and ag not in vs_ag and ag not in css_ag]\n",
    "\n",
    "                ag_dict = {\"shared\":shared_ag, \"ds_vs_css\":ds_vs_css_ag, \"ds_vs_lss\":ds_vs_lss_ag, \"ds_css_lss\":ds_css_lss_ag, \"vs_css_lss\":vs_css_lss_ag,\n",
    "                        \"ds_vs\":ds_vs_ag, \"ds_css\":ds_css_ag, \"ds_lss\":ds_lss_ag, \"vs_css\":vs_css_ag, \"vs_lss\":vs_lss_ag,\n",
    "                        \"css_lss\":css_lss_ag, \"ds\":ds_u_ag, \"vs\":vs_u_ag, \"css\":css_u_ag, \"lss\":lss_u_ag}\n",
    "\n",
    "                for ag_name, ag_list in ag_dict.items():\n",
    "                    if len(ag_list) == 0:\n",
    "                        continue\n",
    "                    write_antigens(fw, mapcode, ag_list, ag_name)\n",
    "\n",
    "                fw.write(\"\\n\")\n",
    "                for ag_name, ag_list in ag_dict.items():\n",
    "                    if len(ag_list) == 0:\n",
    "                        continue\n",
    "                    fw.write(f\"agSize(map{mapcode})[{ag_name}] <- 8\\n\")\n",
    "                    fw.write(f\"agFill(map{mapcode})[{ag_name}] <- '{ag_colors[ag_name]}'\\n\") \n",
    "\n",
    "                li = list(set(ds_ag+vs_ag+lss_ag+css_ag))\n",
    "                fw.write(f\"ptDrawingOrder(map{mapcode}) <- c(seq_len(numSera(map{mapcode})) + numAntigens(map{mapcode}),\\n\")\n",
    "                if len(li) <= 10:\n",
    "                    fw.write(f\"    which(!agNames(map{mapcode}) %in% c{tuple(li)}),\\n\")\n",
    "                    fw.write(f\"    which(agNames(map{mapcode}) %in% c{tuple(li)})\\n\")\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    l = ','.join([f\"'{ag}'\" for ag in li[0:10]])\n",
    "                    fw.write(f\"\\twhich(!agNames(map{mapcode}) %in% c({l},\\n\")\n",
    "                    if len(range(10, len(li)-10, 10)) > 0:\n",
    "                        for i in range(10, len(li)-10, 10):\n",
    "                            l = ','.join([f'\"{ag}\"' for ag in li[i:i+10]])\n",
    "                            if i > len(li)-10:\n",
    "                                fw.write(f\"\\t{l})),\\n\")\n",
    "                            else:\n",
    "                                fw.write(f\"\\t{l},\\n\")\n",
    "                        if i < len(li)-10:\n",
    "                            l = ','.join([f'\"{ag}\"' for ag in li[i+10:]])\n",
    "                            fw.write(f\"\\t{l})),\\n\")\n",
    "                    else:\n",
    "                        l = ','.join([f'\"{ag}\"' for ag in li[10:]])\n",
    "                        fw.write(f\"\\t{l})),\\n\")\n",
    "\n",
    "\n",
    "                    l = ','.join([f\"'{ag}'\" for ag in li[0:10]])\n",
    "                    fw.write(f\"\\twhich(agNames(map{mapcode}) %in% c({l},\")\n",
    "                    if len(range(10, len(li)-10, 10)) > 0:\n",
    "                        for i in range(10, len(li)-10, 10):\n",
    "                            l = ','.join([f'\"{ag}\"' for ag in li[i:i+10]])\n",
    "                            if i > len(li)-10:\n",
    "                                fw.write(f\"\\t{l}))\\n\")\n",
    "                            else:\n",
    "                                fw.write(f\"\\t{l},\")\n",
    "                        if i < len(li)-10:\n",
    "                            l = ','.join([f'\"{ag}\"' for ag in li[i+10:]])\n",
    "                            fw.write(f\"\\t{l}))\\n\")\n",
    "                    else:\n",
    "                        l = ','.join([f'\"{ag}\"' for ag in li[10:]])\n",
    "                        fw.write(f\"\\t{l}))\\n\")\n",
    "                fw.write(\")\\n\")\n",
    "\n",
    "                fw.write(f\"p_map{mapcode}_{region}_{season.replace('-','_')} <- ggplot(map{mapcode}) + ggtitle('{region} {season}  (map {mapcode})')\\n\")\n",
    "                fw.write(f\"p_map{mapcode}_{region}_{season.replace('-','_')}\\n\\n\")\n",
    "                fw.write(f\"p_map{mapcode}_{region}_{season.replace('-','_')}_png <- file.path(strain_fig_dir, 'map{mapcode}_{region}_{season.replace('-','_')}.png')\\n\")\n",
    "                fw.write(f\"png(file=p_map{mapcode}_{region}_{season.replace('-','_')}_png)\\n\")\n",
    "                fw.write(f\"p_map{mapcode}_{region}_{season.replace('-','_')}\\n\")\n",
    "                fw.write(\"dev.off()\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Remove low quality maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_quality_maps = [7,8,9,10,33,59,71,72,86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>season</th>\n",
       "      <th>table list</th>\n",
       "      <th>map code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aunz</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003_table5, 2003_table6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aunz</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003_table5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aunz</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003_table6</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aunz</td>\n",
       "      <td>2004</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aunz</td>\n",
       "      <td>2005</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>us</td>\n",
       "      <td>2020-2021</td>\n",
       "      <td>Sep2021_table7-4, Sep2020_table7-5</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>us</td>\n",
       "      <td>2021-2022</td>\n",
       "      <td>Feb2021_table7-2, Feb2020_table7-5, Sep2022_ta...</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>us</td>\n",
       "      <td>2022-2023</td>\n",
       "      <td>Sep2021_table7-6, Sep2022_table10-19</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>us</td>\n",
       "      <td>2022-2023</td>\n",
       "      <td>Sep2021_table7-6, Sep2022_table10-17, Sep2022_...</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>us</td>\n",
       "      <td>2022-2023</td>\n",
       "      <td>Sep2021_table7-6, Feb2023_tableH3-10, Feb2023_...</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    region     season                                         table list  \\\n",
       "0     aunz       2003                           2003_table5, 2003_table6   \n",
       "1     aunz       2003                                        2003_table5   \n",
       "2     aunz       2003                                        2003_table6   \n",
       "3     aunz       2004                                                      \n",
       "4     aunz       2005                                                      \n",
       "..     ...        ...                                                ...   \n",
       "200     us  2020-2021                 Sep2021_table7-4, Sep2020_table7-5   \n",
       "201     us  2021-2022  Feb2021_table7-2, Feb2020_table7-5, Sep2022_ta...   \n",
       "202     us  2022-2023               Sep2021_table7-6, Sep2022_table10-19   \n",
       "203     us  2022-2023  Sep2021_table7-6, Sep2022_table10-17, Sep2022_...   \n",
       "204     us  2022-2023  Sep2021_table7-6, Feb2023_tableH3-10, Feb2023_...   \n",
       "\n",
       "     map code  \n",
       "0         1.0  \n",
       "1         2.0  \n",
       "2         3.0  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "..        ...  \n",
       "200     160.0  \n",
       "201     161.0  \n",
       "202     125.0  \n",
       "203     162.0  \n",
       "204     163.0  \n",
       "\n",
       "[205 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_HI_table_selection.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (region, season) in closest_HI_strains.index.unique():\n",
    "    if (region, season) in manual_HI_table_selection.sort_index().index.to_list():\n",
    "        continue\n",
    "    print (f\"('{region}', '{season}')\")\n",
    "\n",
    "    sdf = closest_HI_strains.loc[(region, season), :].set_index(\"strain\")\n",
    "    HI_antigen_strains = [s for sl in sdf[\"HI antigens\"].tolist() for s in sl.split(\", \")]\n",
    "    HI_antigen_tables = HI_titers[HI_titers[\"strain\"].isin(HI_antigen_strains)][[\"table\", \"type\", \"strain\"]].drop_duplicates(ignore_index=True)\n",
    "\n",
    "    for t in HI_antigen_tables[\"type\"].unique():\n",
    "        tdf = HI_antigen_tables[HI_antigen_tables[\"type\"]==t]\n",
    "\n",
    "        #determine antigens for each strain\n",
    "        ds = tdf[tdf[\"strain\"].isin(sdf.loc[\"dominant strain\", \"HI antigens\"].split(\", \"))][\"strain\"].unique().tolist()\n",
    "        \n",
    "        vs = tdf[tdf[\"strain\"].isin(sdf.loc[\"vaccine strain\", \"HI antigens\"].split(\", \"))][\"strain\"].unique().tolist()\n",
    "        css = tdf[tdf[\"strain\"].isin(sdf.loc[\"reproducible selection at WHO-timing\", \"HI antigens\"].split(\", \"))][\"strain\"].unique().tolist()\n",
    "        lss = tdf[tdf[\"strain\"].isin(sdf.loc[\"reproducible selection at delayed timing\", \"HI antigens\"].split(\", \"))][\"strain\"].unique().tolist()\n",
    "\n",
    "        #if no antigens for any of the strains continue\n",
    "        if len(ds)==0 or len(vs)==0 or len(css)==0 or len(lss)==0:\n",
    "            continue \n",
    "\n",
    "        #for each strain determine in which table they are\n",
    "        ds_tables, vs_tables, css_tables, lss_tables = {}, {}, {},{}\n",
    "        for tb in tdf[\"table\"].unique():\n",
    "            ts = tdf[tdf[\"table\"]==tb][\"strain\"].unique().tolist()\n",
    "        \n",
    "            ds_tables[tb] = [s for s in ts if s in  ds]\n",
    "            vs_tables[tb] = [s for s in ts if s in  vs]\n",
    "            css_tables[tb] = [s for s in ts if s in css]\n",
    "            lss_tables[tb] = [s for s in ts if s in lss]\n",
    "\n",
    "        #remove empty tables\n",
    "        ds_tables = {k:v for k, v in ds_tables.items() if len(v)>0}\n",
    "        vs_tables = {k:v for k, v in vs_tables.items() if len(v)>0}\n",
    "        css_tables = {k:v for k, v in css_tables.items() if len(v)>0}\n",
    "        lss_tables = {k:v for k, v in lss_tables.items() if len(v)>0}\n",
    "\n",
    "        #make an overview of the tables and the number of strain is each table\n",
    "        table_overview = []\n",
    "        for i, tb in enumerate(set(list(ds_tables.keys()) + list(vs_tables.keys()) + list(css_tables.keys())+list(lss_tables.keys())), start=1):\n",
    "            l = []\n",
    "            l.append(len(ds_tables[tb]) if tb in ds_tables else 0)\n",
    "            l.append(len(vs_tables[tb]) if tb in vs_tables else 0)\n",
    "            l.append(len(css_tables[tb]) if tb in css_tables else 0)\n",
    "            l.append(len(lss_tables[tb]) if tb in lss_tables else 0)\n",
    "            table_overview.append([i, tb]+l)\n",
    "\n",
    "        table_overview = pd.DataFrame.from_records(table_overview, columns=[\"tn\", \"table\", \"ds\", \"vs\", \"css\", \"lss\"])\n",
    "\n",
    "        print (f\"ds: {len(ds)}, vs: {len(vs)}, css: {len(css)}, lss: {len(lss)} \")\n",
    "        table_overview = table_overview.set_index(\"table\").sort_index()\n",
    "        table_overview.index.name = None\n",
    "        print(table_overview[[\"ds\", \"vs\", \"css\", \"lss\"]].to_string())\n",
    "\n",
    "                \n",
    "        ds_, vs_, css_, lss_= ds, vs, css,lss\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get centroid distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    selected_HI_antigens = selected_HI_antigens.set_index([\"region\", \"season\", \"map code\"]).sort_index()\n",
    "except:\n",
    "    selected_HI_antigens = selected_HI_antigens.reset_index().set_index([\"region\", \"season\", \"map code\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/nj8tf3tj6gv6m18wpfp2c96m0000gn/T/ipykernel_87718/2802703632.py:53: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  centroid_distances.loc[(region, season), :] = [pd.NA]*len(centroid_distances.columns)\n",
      "/var/folders/9d/nj8tf3tj6gv6m18wpfp2c96m0000gn/T/ipykernel_87718/2802703632.py:53: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  centroid_distances.loc[(region, season), :] = [pd.NA]*len(centroid_distances.columns)\n",
      "/var/folders/9d/nj8tf3tj6gv6m18wpfp2c96m0000gn/T/ipykernel_87718/2802703632.py:53: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  centroid_distances.loc[(region, season), :] = [pd.NA]*len(centroid_distances.columns)\n",
      "/var/folders/9d/nj8tf3tj6gv6m18wpfp2c96m0000gn/T/ipykernel_87718/2802703632.py:53: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  centroid_distances.loc[(region, season), :] = [pd.NA]*len(centroid_distances.columns)\n",
      "/var/folders/9d/nj8tf3tj6gv6m18wpfp2c96m0000gn/T/ipykernel_87718/2802703632.py:53: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  centroid_distances.loc[(region, season), :] = [pd.NA]*len(centroid_distances.columns)\n",
      "/var/folders/9d/nj8tf3tj6gv6m18wpfp2c96m0000gn/T/ipykernel_87718/2802703632.py:53: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  centroid_distances.loc[(region, season), :] = [pd.NA]*len(centroid_distances.columns)\n"
     ]
    }
   ],
   "source": [
    "centroid_distances = []\n",
    "coord_dir = \"../analysis/antigenic_comparison/antigenic_maps/coords\"\n",
    "\n",
    "for (region, season, mapcode) in selected_HI_antigens.index.unique():\n",
    "    if mapcode in low_quality_maps or pd.isna(mapcode):\n",
    "        continue\n",
    "    df = selected_HI_antigens.loc[(region, season, mapcode),].set_index(\"strain\").sort_index()\n",
    "    tl = df[\"table list\"].values[0].split(\", \")\n",
    "\n",
    "    #get antigens of the strains\n",
    "    ds = df.loc[\"dominant strain\", \"selected antigens\"].split(\", \")\n",
    "    ds_ag = HI_titers[(HI_titers[\"table\"].isin(tl))&(HI_titers[\"strain\"].isin(ds))][\"antigen\"].unique().tolist()\n",
    "    vs= df.loc[\"vaccine strain\", \"selected antigens\"].split(\", \")\n",
    "    vs_ag = HI_titers[(HI_titers[\"table\"].isin(tl))&(HI_titers[\"strain\"].isin(vs))][\"antigen\"].unique().tolist()\n",
    "    vs_ag = [ag.replace(\",\", \"\") for ag in vs_ag]\n",
    "    css= df.loc[\"reproducible selection at WHO-timing\", \"selected antigens\"].split(\", \")\n",
    "    css_ag = HI_titers[(HI_titers[\"table\"].isin(tl))&(HI_titers[\"strain\"].isin(css))][\"antigen\"].unique().tolist()\n",
    "    lss= df.loc[\"reproducible selection at delayed timing\", \"selected antigens\"].split(\", \")\n",
    "    lss_ag = HI_titers[(HI_titers[\"table\"].isin(tl))&(HI_titers[\"strain\"].isin(lss))][\"antigen\"].unique().tolist()\n",
    "\n",
    "    \n",
    "    #get coordinates\n",
    "    coords = pd.read_csv(os.path.join(coord_dir, f\"map{int(mapcode)}.csv\"), names=[\"type\", \"antigen\", \"X\", \"Y\"], header=0)\n",
    "    #filter antigens\n",
    "    coords = coords[coords['type']==\"antigen\"].set_index(\"antigen\").sort_index().drop(columns=\"type\")\n",
    "\n",
    "    #get centriod distances\n",
    "    ds_coords = coords[coords.index.isin(ds_ag)].values.tolist()\n",
    "    ds_centroid = centroid(ds_coords)\n",
    "\n",
    "    vs_coords = coords[coords.index.isin(vs_ag)].values.tolist()\n",
    "    vs_centroid = centroid(vs_coords)\n",
    "\n",
    "    css_coords = coords[coords.index.isin(css_ag)].values.tolist()\n",
    "    css_centroid = centroid(css_coords)\n",
    "\n",
    "    lss_coords = coords[coords.index.isin(lss_ag)].values.tolist()\n",
    "    lss_centroid = centroid(lss_coords)\n",
    "\n",
    "    #get distances  \n",
    "    ds2vs = cdist([ds_centroid], [vs_centroid])[0][0]\n",
    "    ds2css = cdist([ds_centroid], [css_centroid])[0][0]\n",
    "    ds2lss = cdist([ds_centroid], [lss_centroid])[0][0]\n",
    "\n",
    "    centroid_distances.append([region, season, mapcode, ds2vs, ds2css, ds2lss, len(ds_coords), len(vs_coords), len(css_coords), len(lss_coords)])\n",
    "\n",
    "centroid_distances = pd.DataFrame.from_records(centroid_distances, columns=[\"region\", \"season\", \"map code\", \"DS to VS\", \"DS to CSS\", \"DS to LSS\", \"DS cluster size\", \n",
    "                                                                            \"VS cluster size\", \"CSS cluster size\", \"LSS cluster size\"]).set_index([\"region\", \"season\"]).sort_index()\n",
    "\n",
    "for region, h in region_hemispheres.items():\n",
    "    for season, (fss, fse) in flu_seasons[h].items():\n",
    "        if (region, season) not in centroid_distances.index.tolist():\n",
    "            centroid_distances.loc[(region, season), :] = [pd.NA]*len(centroid_distances.columns)\n",
    "centroid_distances = centroid_distances.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_distances.reset_index().to_csv(\"../analysis/antigenic_comparison/centroid_distances.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_centroid_distances = []\n",
    "summary_centroid_distances = []\n",
    "for (region, season) in centroid_distances.index.unique():\n",
    "    df = centroid_distances.loc[(region, season)].dropna()\n",
    "    if len(df) == 0:\n",
    "        summary_centroid_distances.append([region, season,pd.NA,pd.NA,pd.NA])\n",
    "        for c in  [\"DS to VS\", \"DS to CSS\", \"DS to LSS\"]:\n",
    "            median_centroid_distances.append([region, season,c, pd.NA,pd.NA,pd.NA])\n",
    "\n",
    "        continue\n",
    "    \n",
    "    sd = []\n",
    "    for c in  [\"DS to VS\", \"DS to CSS\", \"DS to LSS\"]:\n",
    "        median = round(np.median(df[c]),3)\n",
    "        l_iqr = round(np.quantile(df[c], 0.25),3)\n",
    "        u_iqr = round(np.quantile(df[c], 0.75),3)\n",
    "\n",
    "        sd.append(median)\n",
    "        median_centroid_distances.append([region, season, c, median, l_iqr, u_iqr])\n",
    "\n",
    "    summary_centroid_distances.append([region, season]+sd)\n",
    "\n",
    "median_centroid_distances = pd.DataFrame.from_records(median_centroid_distances, columns=[\"region\", \"season\", \"comparison\", \"median\", \"lower IQR\", \"upper IQR\"]).set_index([\"region\", \"season\"]).sort_index()\n",
    "summary_centroid_distances = pd.DataFrame.from_records(summary_centroid_distances, columns=[\"region\", \"season\", \"DS to VS\", \"DS to CSS\", \"DS to LSS\"]).set_index([\"region\", \"season\"]).sort_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
