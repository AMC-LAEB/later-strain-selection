{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible strain selection based on 2-global AA consensus for Northern and Southern Hemispheres\n",
    "\n",
    "**Assumptions:**\n",
    "- Northern hemisphere: \n",
    "  - flu season starts in October of year X and last through April of subsequent year X+1, the vaccine for this season is picked in February of year X\n",
    "  - 2011/2012 season is the cutoff from using HA1 sequence, using complete sequences afterwards\n",
    "- Southern hemisphere: \n",
    "  - flu season starts in March of year X and last through September of year X, the vaccine for this season in picked in September of the preceding year X-1\n",
    "  -  2011 season is the cutoff from using HA1 sequence, using complete sequences afterwards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dendropy, math, sys\n",
    "import pandas as pd, numpy as np\n",
    "from Bio import SeqIO\n",
    "import calendar\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "sys.path.insert(1, \"../scripts\")\n",
    "\n",
    "from labels import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 general variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_mature_protein = 17\n",
    "start_mature_protein_nuc = 17*3\n",
    "\n",
    "HA1_length_AA = 329 #in mature protein \n",
    "HA1_length_nuc= 329*3\n",
    "\n",
    "protein_length = 567\n",
    "sequence_length = 1701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_hemispheres = {\"us\":\"nh\", \"europe\":\"nh\", \"aunz\":\"sh\"}\n",
    "hemispheres = [\"nh\", \"sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although vaccine strain selection happens some time late february or september, setting vaccine strain selection to the first of the month for coding convienence > working with months mostly anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 24 #preceding period in months\n",
    "pdur = 16 #period duration\n",
    "flu_seasons = {h:{} for h in hemispheres}\n",
    "vaccine_selection = {h:{} for h in hemispheres}\n",
    "for y in range(2002, 2024):\n",
    "    #northern hemisphere flu season\n",
    "    season = f\"{y}-{y+1}\"\n",
    "    if y != 2023:\n",
    "        flu_seasons[\"nh\"][season] = (date(y, 10, 1), date(y+1, 4, 30))\n",
    "        #northern hemisphere vaccine strain selection moment\n",
    "        vaccine_selection[\"nh\"][season] = date(y,2,1)  \n",
    "    \n",
    "    if y==2002:\n",
    "        continue\n",
    "    #southern hemisphere flu season\n",
    "    flu_seasons[\"sh\"][str(y)] = (date(y, 3, 1), date(y, 9, 30))\n",
    "    #southern hemisphere vaccine strain selection moment\n",
    "    vaccine_selection[\"sh\"][str(y)] = date(y-1,9,1)\n",
    "\n",
    "season_periods = {h:{} for h in hemispheres}\n",
    "period_dates = {}\n",
    "for h, sd in flu_seasons.items():\n",
    "    for season, (fss, fse) in sd.items():\n",
    "        vsd = vaccine_selection[h][season]\n",
    "        #something with periods in treason doesn't seem to be correct but don't wanna waste time on that for now\n",
    "        if h == \"nh\":\n",
    "            ps, pe  = vsd + relativedelta(months=-pp), vsd + relativedelta(months=+pdur-1)\n",
    "        else:\n",
    "            ps, pe  = vsd + relativedelta(months=-pp), vsd + relativedelta(months=+pdur-1)\n",
    "\n",
    "        \n",
    "        period = f\"{str(ps.year)[2:]}{'0'+str(ps.month) if len(str(ps.month))==1 else str(ps.month)}-{str(pe.year)[2:]}{'0'+str(pe.month) if len(str(pe.month))==1 else str(pe.month)}\"\n",
    "        season_periods[h][season] = period\n",
    "        period_dates[period] = (ps, pe)\n",
    "\n",
    "#cut off for HA1 \n",
    "early_season_cutoff = {'nh':'2011-2012', 'sh':'2011'}\n",
    "early_season_cutoff_dates = {h:flu_seasons[h][season][-1] for h, season in early_season_cutoff.items()}\n",
    "early_seasons = [] #get list\n",
    "for h, cutoff in early_season_cutoff.items():\n",
    "    csons = list(flu_seasons[h].keys())\n",
    "    for i, season in enumerate(csons):\n",
    "        if i <= csons.index(cutoff):\n",
    "            early_seasons.append(season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sequence_length(segment, sequence, mnlp=0.95, mnsl=None):\n",
    "    \"\"\"check if sequence is smaller than {mnlp}% of the reference sequence for the segment\"\"\"\n",
    "    #for reference_1968 \n",
    "    reference_lengths = {'PB2':2280,'PB1':2274,'PA':2151,'HA':1701,'NP':1497,'NA':1410,'M':890,'NS':800}\n",
    "    if mnsl is None:\n",
    "        if len(sequence) < np.round(reference_lengths[segment]*mnlp):\n",
    "            return False \n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        if len(sequence) < mnsl:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "def check_max_ambig(sequence, mxa=0.01):\n",
    "    \"\"\"check if sequence has ambiguous nucleotide % greater than the mxa\"\"\"\n",
    "    #also checking if there aren't any illegal characters present\n",
    "    valid_nucs = ['A','T','C','G','R','Y','B','D','K','M','H','V','S','W','N']\n",
    "    if not all(i in valid_nucs for i in sequence.upper()):\n",
    "        return False\n",
    "\n",
    "    N_ambig = sequence.upper().count(\"N\")\n",
    "    \n",
    "    if round(N_ambig/len(sequence),2) > mxa:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def get_consensus_sequence(sequences):\n",
    "    seqs = []\n",
    "    for seq in sequences:\n",
    "        if type(seq) == list:\n",
    "            seqs.append(seq)\n",
    "        else:\n",
    "            seqs.append(list(seq))\n",
    "    seqs = pd.DataFrame.from_records(seqs)#.reset_index(drop=)\n",
    "    consensus = seqs.mode().iloc[0,:].to_list()\n",
    "    return \"\".join(consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data prep \n",
    "prepare raw GISAID data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gisaid_raw_data_folder = \"../data/gisaid_data/raw_downloads/H3N2_HA\"\n",
    "outliers = pd.read_csv(\"../data/outliers/H3N2_HA_gitr.csv\")[\"Isolate_Id\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sequences downloaded from GISAID: 151892\n",
      "number of sequence with unknown country of origin 20.\t (Sequences were removed)\n",
      "number of sequences that are known outliers: 3014.\t (Sequences were removed)\n",
      "number of sequences with illegal characters in their sequence: 123.\t (Sequences were removed)\n"
     ]
    }
   ],
   "source": [
    "#get raw data\n",
    "sequences, metadata= {}, []\n",
    "for f in os.listdir(gisaid_raw_data_folder):\n",
    "    if \"010199\" in f:\n",
    "        continue #dont need data from 99\n",
    "    if f.endswith(\".fasta\"):\n",
    "        for r in SeqIO.parse(os.path.join(gisaid_raw_data_folder, f), \"fasta\"):\n",
    "            sequences[r.id] = r.seq\n",
    "    elif f.endswith(\".xls\"):\n",
    "        try:\n",
    "            metadata = pd.concat([metadata, pd.read_excel(os.path.join(gisaid_raw_data_folder, f), usecols=metcols)])\n",
    "            metadata = metadata.drop_duplicates().reset_index(drop=True)\n",
    "        except:\n",
    "            metadata = pd.read_excel(os.path.join(gisaid_raw_data_folder, f), usecols=metcols)\n",
    "\n",
    "#parse collection date as date\n",
    "metadata[\"Collection_Date\"] =pd.to_datetime(metadata[\"Collection_Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "#deterime passage history \n",
    "for i, row in metadata.iterrows():\n",
    "    ph = row[\"Passage_History\"]\n",
    "    if ph in cpl:\n",
    "        metadata.loc[i, \"Passage_History\"] = \"clinical\"\n",
    "    elif ph in cbpl:\n",
    "        metadata.loc[i, \"Passage_History\"] = \"cell-based MDCK or SIAT\"\n",
    "    elif ph in epl:\n",
    "        metadata.loc[i, \"Passage_History\"] = \"egg-based\"\n",
    "    elif ph in ocbpl:\n",
    "        metadata.loc[i, \"Passage_History\"] = \"cell-based other\"\n",
    "    else:\n",
    "        metadata.loc[i, \"Passage_History\"] = \"unknown or unclear\"\n",
    "\n",
    "#determine hemisphere and country\n",
    "for i, row in metadata.iterrows():\n",
    "    l = row[\"Location\"]\n",
    "    #determine country\n",
    "    try:\n",
    "        country = l.split(\" / \")[1]\n",
    "    except:\n",
    "        if l == \"Sudan, South\":\n",
    "            country == \"sudan\"\n",
    "        else:\n",
    "            country = pd.NA\n",
    "    country = cs[country] if country in cs.keys() and not pd.isna(country) else country\n",
    "    if not pd.isna(country) and country.upper() == country:\n",
    "        country = country.lower().capitalize()\n",
    "    metadata.loc[i, \"country\"] = country\n",
    "    \n",
    "    #determine hemisphere\n",
    "    if not pd.isna(country):\n",
    "        if country in nhc:\n",
    "            metadata.loc[i, \"hemisphere\"] = \"northern\"\n",
    "        elif country in shc:\n",
    "            metadata.loc[i, \"hemisphere\"] = \"southern\"\n",
    "        else:\n",
    "            print (country)\n",
    "            metadata.loc[i, \"hemisphere\"] = pd.NA\n",
    "    else:\n",
    "        metadata.loc[i, \"hemisphere\"] = pd.NA\n",
    "\n",
    "#sort by collection date\n",
    "metadata = metadata.sort_values([\"Collection_Date\"]).reset_index(drop=True)\n",
    "\n",
    "#remove sequences with unknown country \n",
    "print (f\"total number of sequences downloaded from GISAID: {len(metadata)}\")\n",
    "print (f\"number of sequence with unknown country of origin {len(metadata) - len(metadata.dropna(subset='country'))}.\\t (Sequences were removed)\")\n",
    "metadata = metadata.dropna(subset='country').reset_index(drop=True)     \n",
    "\n",
    "found_outliers = []\n",
    "for sid in metadata[\"Isolate_Id\"].unique():\n",
    "    if sid in outliers:\n",
    "        found_outliers.append(sid)\n",
    "        continue\n",
    "print (f\"number of sequences that are known outliers: {len(found_outliers)}.\\t (Sequences were removed)\")\n",
    "# check for valid nucleotides in sequences\n",
    "valid_nucs = ['A','T','C','G','R','Y','B','D','K','M','H','V','S','W','N']\n",
    "weird_nucs = []\n",
    "for sid, seq in sequences.items():\n",
    "    if not all(i in valid_nucs for i in seq.upper()):\n",
    "        weird_nucs.append(sid.split(\"|\")[0])\n",
    "print (f\"number of sequences with illegal characters in their sequence: {len(weird_nucs)}.\\t (Sequences were removed)\")\n",
    "# metadata = metadata[~metadata[\"Isolate_Id\"].isin(found_outliers + weird_nucs)]\n",
    "metadata = metadata[~metadata[\"Isolate_Id\"].isin(weird_nucs)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Align sequences\n",
    "check how many sequences there are with complete (HA1) nucleotide sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make alignment \n",
    "alignment = \"../data/gisaid_data/gisaid_2000_2023_H3N2_HA_aligment.fasta\"\n",
    "threads = 12\n",
    "ref_ha = \"../data/references/H3N2_HA.fasta\"\n",
    "\n",
    "if not os.path.isfile(alignment):\n",
    "    seq_file = \"../data/gisaid_data/gisaid_2000_2023_H3N2_HA_filtered.fasta\"\n",
    "    with open(seq_file, \"w\") as fw:\n",
    "        for sid, seq in sequences.items():\n",
    "            fw.write(f\">{sid}\\n{seq}\\n\")\n",
    "\n",
    "    cmd = ['mafft', '--auto', '--thread', str(threads), '--keeplength', '--addfragments', seq_file , ref_ha,'>', alignment]\n",
    "    os.system(\" \".join(cmd))\n",
    "\n",
    "aligned_seqs = {r.id.split(\"|\")[0]:r.seq for r in list(SeqIO.parse(alignment, \"fasta\"))[1:]} \n",
    "aligned_ref = list(SeqIO.parse(alignment, \"fasta\"))[0].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sequences that cover HA1\n",
    "HA1_seqids = []\n",
    "for sid, seq in aligned_seqs.items():\n",
    "    HA1_seq = seq[start_mature_protein_nuc -1:start_mature_protein_nuc +HA1_length_nuc-1]\n",
    "    if check_max_ambig(HA1_seq, mxa=0.01):\n",
    "        HA1_seqids.append(sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sequence with complete (>=95%) sequences\n",
    "complete_seqids = []\n",
    "for sid, seq in aligned_seqs.items():\n",
    "    if check_max_ambig(seq[:1701], mxa=0.01) and check_sequence_length(\"HA\", seq[:1701]):\n",
    "        complete_seqids.append(sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences with complete HA1 nucleotide sequence (len=948) and % ambiguous nucleotides < 1% : 137599\n",
      "\t- clinical samples: 62706\n",
      "\t- cell-based (MDCK or SIAT) samples: 32642\n",
      "\t- cell-based (other) samples: 10763\n",
      "\t- egg-based samples: 1104\n",
      "\t- unknown or unclear samples: 30384\n",
      "number of sequences with complete nucleotide sequence (len=1701) and % ambiguous nucleotides < 1% : 121410\n",
      "\t- clinical samples: 59143\n",
      "\t- cell-based (MDCK or SIAT) samples: 30258\n",
      "\t- cell-based (other) samples: 10024\n",
      "\t- egg-based samples: 996\n",
      "\t- unknown or unclear samples: 20989\n"
     ]
    }
   ],
   "source": [
    "#print results\n",
    "print (f\"number of sequences with complete HA1 nucleotide sequence (len=948) and % ambiguous nucleotides < 1% : {len(HA1_seqids)}\")\n",
    "print (f\"\\t- clinical samples: {len(metadata[(metadata['Passage_History']=='clinical')&(metadata['Isolate_Id'].isin(HA1_seqids))])}\")\n",
    "print (f\"\\t- cell-based (MDCK or SIAT) samples: {len(metadata[(metadata['Passage_History']=='cell-based MDCK or SIAT')&(metadata['Isolate_Id'].isin(HA1_seqids))])}\")\n",
    "print (f\"\\t- cell-based (other) samples: {len(metadata[(metadata['Passage_History']=='cell-based other')&(metadata['Isolate_Id'].isin(HA1_seqids))])}\")\n",
    "print (f\"\\t- egg-based samples: {len(metadata[(metadata['Passage_History']=='egg-based')&(metadata['Isolate_Id'].isin(HA1_seqids))])}\")\n",
    "print (f\"\\t- unknown or unclear samples: {len(metadata[(metadata['Passage_History']=='unknown or unclear')&(metadata['Isolate_Id'].isin(HA1_seqids))])}\")\n",
    "\n",
    "print (f\"number of sequences with complete nucleotide sequence (len=1701) and % ambiguous nucleotides < 1% : {len(complete_seqids)}\")\n",
    "print (f\"\\t- clinical samples: {len(metadata[(metadata['Passage_History']=='clinical')&(metadata['Isolate_Id'].isin(complete_seqids))])}\")\n",
    "print (f\"\\t- cell-based (MDCK or SIAT) samples: {len(metadata[(metadata['Passage_History']=='cell-based MDCK or SIAT')&(metadata['Isolate_Id'].isin(complete_seqids))])}\")\n",
    "print (f\"\\t- cell-based (other) samples: {len(metadata[(metadata['Passage_History']=='cell-based other')&(metadata['Isolate_Id'].isin(complete_seqids))])}\")\n",
    "print (f\"\\t- egg-based samples: {len(metadata[(metadata['Passage_History']=='egg-based')&(metadata['Isolate_Id'].isin(complete_seqids))])}\")\n",
    "print (f\"\\t- unknown or unclear samples: {len(metadata[(metadata['Passage_History']=='unknown or unclear')&(metadata['Isolate_Id'].isin(complete_seqids))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to metadata\n",
    "for i, row in metadata.iterrows():\n",
    "    seqid = row[\"Isolate_Id\"]\n",
    "    if seqid in complete_seqids:\n",
    "        metadata.loc[i, \"sequence_length\"] = \"complete\"\n",
    "    elif seqid in HA1_seqids:\n",
    "        metadata.loc[i, \"sequence_length\"] = \"HA1\"\n",
    "    else:\n",
    "        metadata.loc[i, \"sequence_length\"] = \"incomplete\"\n",
    "\n",
    "metadata = metadata[metadata[\"sequence_length\"]!=\"incomplete\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reproducible strain selection based on global AA consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter metadata on passage history\n",
    "passage_labels = ['clinical', 'cell-based MDCK or SIAT', 'cell-based other']\n",
    "metadata = metadata[metadata[\"Passage_History\"].isin(passage_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "later_selection_delay = 3 #months\n",
    "\n",
    "global_consensus= []\n",
    "b = False\n",
    "for region, h in region_hemispheres.items():\n",
    "    for season, (fss, fse) in flu_seasons[h].items():\n",
    "\n",
    "        vaccine_strain_selection = vaccine_selection[h][season]\n",
    "\n",
    "        #reproducible selection at current timing\n",
    "        cts = vaccine_strain_selection-relativedelta(months=2)\n",
    "        cte= vaccine_strain_selection-relativedelta(days=1)\n",
    "\n",
    "        lab = f\"{calendar.month_abbr[cts.month]}{str(cts.year)[-2:]}-{calendar.month_abbr[cte.month]}{str(cte.year)[-2:]}\"\n",
    "\n",
    "        seq_ids = metadata[(metadata[\"Collection_Date\"]>=pd.to_datetime(cts))&(metadata[\"Collection_Date\"]<=pd.to_datetime(cte))][\"Isolate_Id\"].tolist()\n",
    "        seqs = [v[:1701].replace(\"-\", \"n\").translate() for k, v in aligned_seqs.items() if k.split(\"|\")[0] in seq_ids]\n",
    "\n",
    "        if season in early_seasons:\n",
    "            seqs = [s[start_mature_protein-1:HA1_length_AA+start_mature_protein-1] for s in seqs]\n",
    "\n",
    "        consensus = get_consensus_sequence(seqs)\n",
    "        observed = False\n",
    "        for s in seqs:\n",
    "            if str(s)==consensus:\n",
    "                observed=True\n",
    "                break\n",
    "\n",
    "\n",
    "        global_consensus.append([region, season, lab, \"WHO-timing\",consensus, observed])\n",
    "\n",
    "        #reproducible selection at delayed timing \n",
    "        dts = cts + relativedelta(months=later_selection_delay)\n",
    "        dte= cte + relativedelta(months=later_selection_delay)\n",
    "\n",
    "        lab = f\"{calendar.month_abbr[dts.month]}{str(dts.year)[-2:]}-{calendar.month_abbr[dte.month]}{str(dte.year)[-2:]}\"\n",
    "\n",
    "        seq_ids = metadata[(metadata[\"Collection_Date\"]>=pd.to_datetime(dts))&(metadata[\"Collection_Date\"]<=pd.to_datetime(dte))][\"Isolate_Id\"].tolist()\n",
    "        seqs = [v[:1701].replace(\"-\", \"n\").translate() for k, v in aligned_seqs.items() if k.split(\"|\")[0] in seq_ids]\n",
    "\n",
    "        if season in early_seasons:\n",
    "            seqs = [s[start_mature_protein-1:HA1_length_AA+start_mature_protein-1] for s in seqs]\n",
    "\n",
    "        consensus = get_consensus_sequence(seqs)\n",
    "        observed = False\n",
    "        for s in seqs:\n",
    "            if str(s)==consensus:\n",
    "                observed=True\n",
    "                break\n",
    "\n",
    "        global_consensus.append([region, season, lab, \"delayed-timing\",consensus, observed])\n",
    "\n",
    "\n",
    "\n",
    "global_consensus = pd.DataFrame.from_records(global_consensus, columns=[\"region\", \"season\", \"months\", \"timing\", \"sequence\", \"observed in global data\"])      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_consensus_file = \"../data/reproducible_selection_strains.fasta\"\n",
    "\n",
    "with open(global_consensus_file, \"w\") as fw:\n",
    "    for i, row in global_consensus.iterrows():\n",
    "        fw.write(f\">{row['region']}_{row['season']}_{row['months']}_{row['timing']}\\n{row['sequence']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
